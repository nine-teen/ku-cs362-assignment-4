{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Linear regression in one dimension\n",
    "class linear_regression_1D:\n",
    "    \n",
    "    # Class constructor\n",
    "    def __init__(self):\n",
    "        # Define the vector w (the slop of the line)\n",
    "        # w is the two diemensional vector becuase we must absorb b into w\n",
    "        self.w = np.zeros(2)\n",
    "        # Define the learning rate\n",
    "        self.alpha = 1\n",
    "\n",
    "    # Class function to set the learning rate\n",
    "    def set_learning_rate(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    # Class function to fit the data (find the appropiate value of w)\n",
    "    def fit(self, x, y, iteration=1500):\n",
    "        self.y = y\n",
    "        # Apply transformation to x. Recall that we must absorb b into w \n",
    "        self.x = np.append(x.transpose(), np.ones((x.shape[1], 1)), axis=1)\n",
    "        # Iteratively make an updage for w by gradient descent\n",
    "        for i in range(iteration):\n",
    "            self.make_one_update()\n",
    "\n",
    "    # Class function to make an update for w\n",
    "    def make_one_update(self):\n",
    "        w_current = self.w\n",
    "        # Set step size \n",
    "        step = (-1)*self.alpha*self.compute_gradient(w_current)\n",
    "        w_update = w_current + step\n",
    "        \n",
    "        # Report the progress of convergence\n",
    "        current_loss = self.sq_loss(w_current)\n",
    "        update_loss = self.sq_loss(w_update)\n",
    "        if current_loss > update_loss:\n",
    "            print(\"Loss decreases to \", update_loss,)\n",
    "        else:\n",
    "            print(\"Loss increases to \", update_loss,)\n",
    "        self.w = w_update\n",
    "    \n",
    "    # Class function to compute the gradient with respect to the current w\n",
    "    def compute_gradient(self, w_current):\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "\n",
    "        n = x.shape[0]\n",
    "\n",
    "        grad_v = np.zeros(2)\n",
    "        # Uncomment the following to implement how to compute the gradient vector \n",
    "        grad_v[0] = (2/n) * sum([(np.inner(w_current, x[k]) - y[k]) * x[k][0] for k in range(n)])\n",
    "        grad_v[1] = (2/n) * sum([(np.inner(w_current, x[k]) - y[k]) * x[k][1] for k in range(n)])\n",
    "        print(\"The norm of grad vector is \", math.sqrt(np.inner(grad_v, grad_v)))\n",
    "        return grad_v\n",
    "\n",
    "    # Class function to compute the square loss function\n",
    "    def sq_loss(self, w):\n",
    "        # Uncomment the following to implement how to compute the loss function\n",
    "        x = self.x\n",
    "        y = self.y\n",
    "\n",
    "        n = x.shape[0]\n",
    "\n",
    "        loss = (1/n) * sum([math.pow(np.inner(w, x[k]) - y[k], 2) for k in range(n)])\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create our data set where x is one-dimensional\n",
    "x = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "y = np.array([5.5, 7.1, 8.7, 13.1, 14.56, 19.01, 19.85, 26.12, 27.11, 28.112])\n",
    "\n",
    "# The 2D plot of our data\n",
    "plt.plot(x.transpose(), y, 'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of grad vector is  234.07392581353437\n",
      "Loss decreases to  17.649887851216004\n",
      "The norm of grad vector is  50.141455471152014\n",
      "Loss decreases to  2.3851912564595934\n",
      "The norm of grad vector is  10.753824388871456\n",
      "Loss decreases to  1.6819191080278229\n",
      "The norm of grad vector is  2.3653124361778195\n",
      "Loss decreases to  1.646770475884871\n",
      "The norm of grad vector is  0.7369045470813277\n",
      "Loss decreases to  1.6423004656138198\n",
      "The norm of grad vector is  0.5557430069944456\n",
      "Loss decreases to  1.6392619834078646\n",
      "The norm of grad vector is  0.5438070212271837\n",
      "Loss decreases to  1.6363129296847854\n",
      "The norm of grad vector is  0.5410767750846707\n",
      "Loss decreases to  1.633391527937782\n",
      "The norm of grad vector is  0.5387839201766682\n",
      "Loss decreases to  1.6304947463981094\n",
      "The norm of grad vector is  0.5365203275541965\n",
      "Loss decreases to  1.6276222502446303\n",
      "The norm of grad vector is  0.5342671456024479\n",
      "Loss decreases to  1.6247738300364567\n",
      "The norm of grad vector is  0.5320234676576439\n",
      "Loss decreases to  1.6219492837119998\n",
      "The norm of grad vector is  0.529789214047213\n",
      "Loss decreases to  1.6191484111612588\n",
      "The norm of grad vector is  0.5275643433613468\n",
      "Loss decreases to  1.6163710139632892\n",
      "The norm of grad vector is  0.5253488161116773\n",
      "Loss decreases to  1.6136168953607424\n",
      "The norm of grad vector is  0.5231425930561875\n",
      "Loss decreases to  1.6108858602454\n",
      "The norm of grad vector is  0.520945635121368\n",
      "Loss decreases to  1.6081777151443413\n",
      "The norm of grad vector is  0.5187579033979703\n",
      "Loss decreases to  1.6054922682062203\n",
      "The norm of grad vector is  0.5165793591401565\n",
      "Loss decreases to  1.6028293291876876\n",
      "The norm of grad vector is  0.5144099637648021\n",
      "Loss decreases to  1.6001887094399045\n",
      "The norm of grad vector is  0.5122496788508157\n",
      "Loss decreases to  1.5975702218951824\n",
      "The norm of grad vector is  0.5100984661384543\n",
      "Loss decreases to  1.5949736810537296\n",
      "The norm of grad vector is  0.507956287528651\n",
      "Loss decreases to  1.5923989029705097\n",
      "The norm of grad vector is  0.5058231050823347\n",
      "Loss decreases to  1.5898457052422161\n",
      "The norm of grad vector is  0.5036988810197642\n",
      "Loss decreases to  1.5873139069943356\n",
      "The norm of grad vector is  0.5015835777198525\n",
      "Loss decreases to  1.5848033288683516\n",
      "The norm of grad vector is  0.49947715771950574\n",
      "Loss decreases to  1.5823137930090323\n",
      "The norm of grad vector is  0.4973795837129575\n",
      "Loss decreases to  1.5798451230518196\n",
      "The norm of grad vector is  0.4952908185511081\n",
      "Loss decreases to  1.5773971441103571\n",
      "The norm of grad vector is  0.49321082524086673\n",
      "Loss decreases to  1.5749696827640765\n",
      "The norm of grad vector is  0.49113956694449756\n",
      "Loss decreases to  1.5725625670459344\n",
      "The norm of grad vector is  0.48907700697896517\n",
      "Loss decreases to  1.5701756264302116\n",
      "The norm of grad vector is  0.4870231088152868\n",
      "Loss decreases to  1.5678086918204446\n",
      "The norm of grad vector is  0.4849778360778833\n",
      "Loss decreases to  1.5654615955374434\n",
      "The norm of grad vector is  0.4829411525439384\n",
      "Loss decreases to  1.5631341713074054\n",
      "The norm of grad vector is  0.48091302214275267\n",
      "Loss decreases to  1.5608262542501494\n",
      "The norm of grad vector is  0.4788934089551092\n",
      "Loss decreases to  1.5585376808674216\n",
      "The norm of grad vector is  0.47688227721263243\n",
      "Loss decreases to  1.5562682890313277\n",
      "The norm of grad vector is  0.47487959129716095\n",
      "Loss decreases to  1.5540179179728275\n",
      "The norm of grad vector is  0.47288531574010995\n",
      "Loss decreases to  1.5517864082703694\n",
      "The norm of grad vector is  0.47089941522184864\n",
      "Loss decreases to  1.5495736018385768\n",
      "The norm of grad vector is  0.4689218545710711\n",
      "Loss decreases to  1.5473793419170598\n",
      "The norm of grad vector is  0.46695259876417633\n",
      "Loss decreases to  1.5452034730593027\n",
      "The norm of grad vector is  0.46499161292464336\n",
      "Loss decreases to  1.5430458411216599\n",
      "The norm of grad vector is  0.4630388623224199\n",
      "Loss decreases to  1.5409062932524327\n",
      "The norm of grad vector is  0.46109431237330184\n",
      "Loss decreases to  1.5387846778810363\n",
      "The norm of grad vector is  0.45915792863832294\n",
      "Loss decreases to  1.5366808447072686\n",
      "The norm of grad vector is  0.45722967682314497\n",
      "Loss decreases to  1.5345946446906595\n",
      "The norm of grad vector is  0.4553095227774513\n",
      "Loss decreases to  1.532525930039914\n",
      "The norm of grad vector is  0.4533974324943395\n",
      "Loss decreases to  1.5304745542024378\n",
      "The norm of grad vector is  0.45149337210972\n",
      "Loss decreases to  1.5284403718539616\n",
      "The norm of grad vector is  0.44959730790171876\n",
      "Loss decreases to  1.5264232388882453\n",
      "The norm of grad vector is  0.44770920629007693\n",
      "Loss decreases to  1.5244230124068618\n",
      "The norm of grad vector is  0.4458290338355559\n",
      "Loss decreases to  1.5224395507090858\n",
      "The norm of grad vector is  0.44395675723935013\n",
      "Loss decreases to  1.5204727132818405\n",
      "The norm of grad vector is  0.4420923433424892\n",
      "Loss decreases to  1.518522360789755\n",
      "The norm of grad vector is  0.4402357591252582\n",
      "Loss decreases to  1.5165883550652937\n",
      "The norm of grad vector is  0.43838697170661\n",
      "Loss decreases to  1.5146705590989578\n",
      "The norm of grad vector is  0.4365459483435804\n",
      "Loss decreases to  1.5127688370295882\n",
      "The norm of grad vector is  0.43471265643071194\n",
      "Loss decreases to  1.5108830541347409\n",
      "The norm of grad vector is  0.43288706349947553\n",
      "Loss decreases to  1.5090130768211385\n",
      "The norm of grad vector is  0.4310691372176952\n",
      "Loss decreases to  1.5071587726152058\n",
      "The norm of grad vector is  0.4292588453889727\n",
      "Loss decreases to  1.50532001015369\n",
      "The norm of grad vector is  0.4274561559521226\n",
      "Loss decreases to  1.5034966591743517\n",
      "The norm of grad vector is  0.42566103698060054\n",
      "Loss decreases to  1.5016885905067352\n",
      "The norm of grad vector is  0.42387345668193893\n",
      "Loss decreases to  1.4998956760630235\n",
      "The norm of grad vector is  0.4220933833971843\n",
      "Loss decreases to  1.4981177888289539\n",
      "The norm of grad vector is  0.4203207856003352\n",
      "Loss decreases to  1.4963548028548317\n",
      "The norm of grad vector is  0.41855563189778616\n",
      "Loss decreases to  1.4946065932466013\n",
      "The norm of grad vector is  0.4167978910277704\n",
      "Loss decreases to  1.4928730361569968\n",
      "The norm of grad vector is  0.4150475318598032\n",
      "Loss decreases to  1.4911540087767772\n",
      "The norm of grad vector is  0.4133045233941388\n",
      "Loss decreases to  1.4894493893260126\n",
      "The norm of grad vector is  0.41156883476121237\n",
      "Loss decreases to  1.4877590570454693\n",
      "The norm of grad vector is  0.40984043522109675\n",
      "Loss decreases to  1.4860828921880476\n",
      "The norm of grad vector is  0.4081192941629603\n",
      "Loss decreases to  1.4844207760103052\n",
      "The norm of grad vector is  0.40640538110452196\n",
      "Loss decreases to  1.4827725907640386\n",
      "The norm of grad vector is  0.4046986656915127\n",
      "Loss decreases to  1.4811382196879403\n",
      "The norm of grad vector is  0.4029991176971356\n",
      "Loss decreases to  1.4795175469993391\n",
      "The norm of grad vector is  0.4013067070215345\n",
      "Loss decreases to  1.4779104578859832\n",
      "The norm of grad vector is  0.3996214036912579\n",
      "Loss decreases to  1.4763168384979173\n",
      "The norm of grad vector is  0.39794317785872835\n",
      "Loss decreases to  1.474736575939407\n",
      "The norm of grad vector is  0.39627199980171574\n",
      "Loss decreases to  1.4731695582609547\n",
      "The norm of grad vector is  0.3946078399228098\n",
      "Loss decreases to  1.471615674451357\n",
      "The norm of grad vector is  0.3929506687488933\n",
      "Loss decreases to  1.4700748144298406\n",
      "The norm of grad vector is  0.3913004569306258\n",
      "Loss decreases to  1.4685468690382804\n",
      "The norm of grad vector is  0.38965717524191873\n",
      "Loss decreases to  1.4670317300334366\n",
      "The norm of grad vector is  0.3880207945794191\n",
      "Loss decreases to  1.4655292900793186\n",
      "The norm of grad vector is  0.3863912859619951\n",
      "Loss decreases to  1.4640394427395598\n",
      "The norm of grad vector is  0.38476862053022476\n",
      "Loss decreases to  1.462562082469886\n",
      "The norm of grad vector is  0.3831527695458785\n",
      "Loss decreases to  1.4610971046106285\n",
      "The norm of grad vector is  0.38154370439141616\n",
      "Loss decreases to  1.459644405379331\n",
      "The norm of grad vector is  0.3799413965694776\n",
      "Loss decreases to  1.4582038818633742\n",
      "The norm of grad vector is  0.37834581770237885\n",
      "Loss decreases to  1.4567754320126989\n",
      "The norm of grad vector is  0.3767569395316084\n",
      "Loss decreases to  1.4553589546325731\n",
      "The norm of grad vector is  0.37517473391732864\n",
      "Loss decreases to  1.4539543493764173\n",
      "The norm of grad vector is  0.37359917283787586\n",
      "Loss decreases to  1.4525615167387071\n",
      "The norm of grad vector is  0.3720302283892639\n",
      "Loss decreases to  1.4511803580479143\n",
      "The norm of grad vector is  0.3704678727846898\n",
      "Loss decreases to  1.449810775459524\n",
      "The norm of grad vector is  0.3689120783540446\n",
      "Loss decreases to  1.448452671949097\n",
      "The norm of grad vector is  0.36736281754342\n",
      "Loss decreases to  1.4471059513053999\n",
      "The norm of grad vector is  0.3658200629146201\n",
      "Loss decreases to  1.4457705181235851\n",
      "The norm of grad vector is  0.3642837871446793\n",
      "Loss decreases to  1.4444462777984384\n",
      "The norm of grad vector is  0.3627539630253734\n",
      "Loss decreases to  1.4431331365176756\n",
      "The norm of grad vector is  0.3612305634627418\n",
      "Loss decreases to  1.4418310012552915\n",
      "The norm of grad vector is  0.3597135614766051\n",
      "Loss decreases to  1.440539779764973\n",
      "The norm of grad vector is  0.3582029302000894\n",
      "Loss decreases to  1.4392593805735634\n",
      "The norm of grad vector is  0.3566986428791478\n",
      "Loss decreases to  1.437989712974588\n",
      "The norm of grad vector is  0.35520067287209073\n",
      "Loss decreases to  1.4367306870218184\n",
      "The norm of grad vector is  0.3537089936491076\n",
      "Loss decreases to  1.4354822135229077\n",
      "The norm of grad vector is  0.35222357879180244\n",
      "Loss decreases to  1.4342442040330676\n",
      "The norm of grad vector is  0.3507444019927265\n",
      "Loss decreases to  1.4330165708488067\n",
      "The norm of grad vector is  0.34927143705490804\n",
      "Loss decreases to  1.4317992270017146\n",
      "The norm of grad vector is  0.34780465789138976\n",
      "Loss decreases to  1.4305920862523003\n",
      "The norm of grad vector is  0.3463440385247702\n",
      "Loss decreases to  1.4293950630838888\n",
      "The norm of grad vector is  0.34488955308673824\n",
      "Loss decreases to  1.4282080726965491\n",
      "The norm of grad vector is  0.3434411758176199\n",
      "Loss decreases to  1.427031031001107\n",
      "The norm of grad vector is  0.3419988810659187\n",
      "Loss decreases to  1.4258638546131674\n",
      "The norm of grad vector is  0.3405626432878636\n",
      "Loss decreases to  1.4247064608472249\n",
      "The norm of grad vector is  0.3391324370469541\n",
      "Loss decreases to  1.4235587677107924\n",
      "The norm of grad vector is  0.3377082370135131\n",
      "Loss decreases to  1.422420693898597\n",
      "The norm of grad vector is  0.3362900179642357\n",
      "Loss decreases to  1.42129215878683\n",
      "The norm of grad vector is  0.33487775478174325\n",
      "Loss decreases to  1.4201730824274135\n",
      "The norm of grad vector is  0.3334714224541373\n",
      "Loss decreases to  1.4190633855423616\n",
      "The norm of grad vector is  0.3320709960745604\n",
      "Loss decreases to  1.4179629895181467\n",
      "The norm of grad vector is  0.3306764508407507\n",
      "Loss decreases to  1.4168718164001364\n",
      "The norm of grad vector is  0.32928776205460464\n",
      "Loss decreases to  1.4157897888870716\n",
      "The norm of grad vector is  0.32790490512173937\n",
      "Loss decreases to  1.4147168303255855\n",
      "The norm of grad vector is  0.32652785555105723\n",
      "Loss decreases to  1.4136528647047832\n",
      "The norm of grad vector is  0.32515658895431254\n",
      "Loss decreases to  1.41259781665085\n",
      "The norm of grad vector is  0.3237910810456777\n",
      "Loss decreases to  1.4115516114217046\n",
      "The norm of grad vector is  0.322431307641315\n",
      "Loss decreases to  1.410514174901721\n",
      "The norm of grad vector is  0.3210772446589485\n",
      "Loss decreases to  1.4094854335964666\n",
      "The norm of grad vector is  0.3197288681174351\n",
      "Loss decreases to  1.408465314627498\n",
      "The norm of grad vector is  0.31838615413634236\n",
      "Loss decreases to  1.4074537457271952\n",
      "The norm of grad vector is  0.3170490789355257\n",
      "Loss decreases to  1.4064506552336509\n",
      "The norm of grad vector is  0.3157176188347042\n",
      "Loss decreases to  1.4054559720855813\n",
      "The norm of grad vector is  0.31439175025304456\n",
      "Loss decreases to  1.404469625817303\n",
      "The norm of grad vector is  0.31307144970874146\n",
      "Loss decreases to  1.4034915465537396\n",
      "The norm of grad vector is  0.31175669381860205\n",
      "Loss decreases to  1.4025216650054608\n",
      "The norm of grad vector is  0.3104474592976346\n",
      "Loss decreases to  1.4015599124637879\n",
      "The norm of grad vector is  0.3091437229586298\n",
      "Loss decreases to  1.4006062207959173\n",
      "The norm of grad vector is  0.30784546171175675\n",
      "Loss decreases to  1.399660522440095\n",
      "The norm of grad vector is  0.3065526525641495\n",
      "Loss decreases to  1.3987227504008395\n",
      "The norm of grad vector is  0.30526527261950387\n",
      "Loss decreases to  1.397792838244179\n",
      "The norm of grad vector is  0.3039832990776662\n",
      "Loss decreases to  1.3968707200929584\n",
      "The norm of grad vector is  0.30270670923423615\n",
      "Loss decreases to  1.395956330622167\n",
      "The norm of grad vector is  0.30143548048015917\n",
      "Loss decreases to  1.3950496050543149\n",
      "The norm of grad vector is  0.3001695903013304\n",
      "Loss decreases to  1.394150479154839\n",
      "The norm of grad vector is  0.29890901627819166\n",
      "Loss decreases to  1.3932588892275541\n",
      "The norm of grad vector is  0.29765373608533857\n",
      "Loss decreases to  1.3923747721101387\n",
      "The norm of grad vector is  0.29640372749112104\n",
      "Loss decreases to  1.3914980651696691\n",
      "The norm of grad vector is  0.295158968357254\n",
      "Loss decreases to  1.3906287062981681\n",
      "The norm of grad vector is  0.29391943663842035\n",
      "Loss decreases to  1.38976663390822\n",
      "The norm of grad vector is  0.29268511038188716\n",
      "Loss decreases to  1.3889117869285947\n",
      "The norm of grad vector is  0.2914559677271077\n",
      "Loss decreases to  1.3880641047999311\n",
      "The norm of grad vector is  0.29023198690534385\n",
      "Loss decreases to  1.387223527470443\n",
      "The norm of grad vector is  0.2890131462392751\n",
      "Loss decreases to  1.3863899953916603\n",
      "The norm of grad vector is  0.28779942414261195\n",
      "Loss decreases to  1.3855634495142195\n",
      "The norm of grad vector is  0.2865907991197234\n",
      "Loss decreases to  1.3847438312836755\n",
      "The norm of grad vector is  0.28538724976524593\n",
      "Loss decreases to  1.3839310826363518\n",
      "The norm of grad vector is  0.2841887547637108\n",
      "Loss decreases to  1.3831251459952307\n",
      "The norm of grad vector is  0.2829952928891635\n",
      "Loss decreases to  1.3823259642658714\n",
      "The norm of grad vector is  0.2818068430047878\n",
      "Loss decreases to  1.3815334808323698\n",
      "The norm of grad vector is  0.2806233840625348\n",
      "Loss decreases to  1.380747639553344\n",
      "The norm of grad vector is  0.27944489510274606\n",
      "Loss decreases to  1.379968384757953\n",
      "The norm of grad vector is  0.27827135525378377\n",
      "Loss decreases to  1.3791956612419656\n",
      "The norm of grad vector is  0.27710274373166244\n",
      "Loss decreases to  1.3784294142638356\n",
      "The norm of grad vector is  0.2759390398396796\n",
      "Loss decreases to  1.377669589540834\n",
      "The norm of grad vector is  0.27478022296804916\n",
      "Loss decreases to  1.3769161332451976\n",
      "The norm of grad vector is  0.27362627259353534\n",
      "Loss decreases to  1.3761689920003133\n",
      "The norm of grad vector is  0.2724771682790933\n",
      "Loss decreases to  1.3754281128769503\n",
      "The norm of grad vector is  0.2713328896735017\n",
      "Loss decreases to  1.374693443389496\n",
      "The norm of grad vector is  0.27019341651100626\n",
      "Loss decreases to  1.373964931492237\n",
      "The norm of grad vector is  0.2690587286109599\n",
      "Loss decreases to  1.3732425255756915\n",
      "The norm of grad vector is  0.2679288058774642\n",
      "Loss decreases to  1.3725261744629278\n",
      "The norm of grad vector is  0.2668036282990133\n",
      "Loss decreases to  1.3718158274059542\n",
      "The norm of grad vector is  0.26568317594814345\n",
      "Loss decreases to  1.371111434082124\n",
      "The norm of grad vector is  0.26456742898107377\n",
      "Loss decreases to  1.3704129445905642\n",
      "The norm of grad vector is  0.2634563676373599\n",
      "Loss decreases to  1.3697203094486399\n",
      "The norm of grad vector is  0.26234997223954337\n",
      "Loss decreases to  1.3690334795884582\n",
      "The norm of grad vector is  0.26124822319279817\n",
      "Loss decreases to  1.3683524063533814\n",
      "The norm of grad vector is  0.2601511009845908\n",
      "Loss decreases to  1.3676770414945867\n",
      "The norm of grad vector is  0.2590585861843314\n",
      "Loss decreases to  1.367007337167647\n",
      "The norm of grad vector is  0.25797065944302777\n",
      "Loss decreases to  1.3663432459291358\n",
      "The norm of grad vector is  0.2568873014929456\n",
      "Loss decreases to  1.3656847207332765\n",
      "The norm of grad vector is  0.25580849314726595\n",
      "Loss decreases to  1.3650317149285998\n",
      "The norm of grad vector is  0.254734215299747\n",
      "Loss decreases to  1.3643841822546476\n",
      "The norm of grad vector is  0.25366444892438195\n",
      "Loss decreases to  1.3637420768386872\n",
      "The norm of grad vector is  0.25259917507506613\n",
      "Loss decreases to  1.3631053531924655\n",
      "The norm of grad vector is  0.2515383748852593\n",
      "Loss decreases to  1.362473966208987\n",
      "The norm of grad vector is  0.2504820295676522\n",
      "Loss decreases to  1.361847871159322\n",
      "The norm of grad vector is  0.24943012041383325\n",
      "Loss decreases to  1.3612270236894313\n",
      "The norm of grad vector is  0.248382628793958\n",
      "Loss decreases to  1.360611379817022\n",
      "The norm of grad vector is  0.2473395361564186\n",
      "Loss decreases to  1.360000895928441\n",
      "The norm of grad vector is  0.24630082402751508\n",
      "Loss decreases to  1.3593955287755837\n",
      "The norm of grad vector is  0.24526647401112933\n",
      "Loss decreases to  1.358795235472816\n",
      "The norm of grad vector is  0.2442364677883971\n",
      "Loss decreases to  1.35819997349396\n",
      "The norm of grad vector is  0.24321078711738667\n",
      "Loss decreases to  1.3576097006692602\n",
      "The norm of grad vector is  0.24218941383277356\n",
      "Loss decreases to  1.3570243751824065\n",
      "The norm of grad vector is  0.24117232984551717\n",
      "Loss decreases to  1.3564439555675711\n",
      "The norm of grad vector is  0.24015951714254488\n",
      "Loss decreases to  1.3558684007064699\n",
      "The norm of grad vector is  0.2391509577864303\n",
      "Loss decreases to  1.3552976698254484\n",
      "The norm of grad vector is  0.23814663391507615\n",
      "Loss decreases to  1.3547317224925912\n",
      "The norm of grad vector is  0.23714652774139663\n",
      "Loss decreases to  1.3541705186148654\n",
      "The norm of grad vector is  0.23615062155300406\n",
      "Loss decreases to  1.3536140184352734\n",
      "The norm of grad vector is  0.23515889771189497\n",
      "Loss decreases to  1.3530621825300404\n",
      "The norm of grad vector is  0.23417133865413686\n",
      "Loss decreases to  1.352514971805816\n",
      "The norm of grad vector is  0.23318792688955886\n",
      "Loss decreases to  1.3519723474969132\n",
      "The norm of grad vector is  0.2322086450014384\n",
      "Loss decreases to  1.3514342711625504\n",
      "The norm of grad vector is  0.23123347564619764\n",
      "Loss decreases to  1.3509007046841468\n",
      "The norm of grad vector is  0.23026240155309274\n",
      "Loss decreases to  1.350371610262596\n",
      "The norm of grad vector is  0.2292954055239075\n",
      "Loss decreases to  1.3498469504156134\n",
      "The norm of grad vector is  0.22833247043265417\n",
      "Loss decreases to  1.349326687975066\n",
      "The norm of grad vector is  0.22737357922526188\n",
      "Loss decreases to  1.3488107860843441\n",
      "The norm of grad vector is  0.22641871491928117\n",
      "Loss decreases to  1.3482992081957468\n",
      "The norm of grad vector is  0.22546786060358243\n",
      "Loss decreases to  1.3477919180679032\n",
      "The norm of grad vector is  0.2245209994380516\n",
      "Loss decreases to  1.347288879763187\n",
      "The norm of grad vector is  0.22357811465329844\n",
      "Loss decreases to  1.3467900576451886\n",
      "The norm of grad vector is  0.22263918955035503\n",
      "Loss decreases to  1.3462954163761822\n",
      "The norm of grad vector is  0.22170420750038097\n",
      "Loss decreases to  1.3458049209146208\n",
      "The norm of grad vector is  0.22077315194437022\n",
      "Loss decreases to  1.3453185365126532\n",
      "The norm of grad vector is  0.21984600639285606\n",
      "Loss decreases to  1.3448362287136737\n",
      "The norm of grad vector is  0.21892275442562215\n",
      "Loss decreases to  1.3443579633498675\n",
      "The norm of grad vector is  0.21800337969140718\n",
      "Loss decreases to  1.3438837065397928\n",
      "The norm of grad vector is  0.21708786590761728\n",
      "Loss decreases to  1.3434134246859895\n",
      "The norm of grad vector is  0.21617619686003936\n",
      "Loss decreases to  1.3429470844725877\n",
      "The norm of grad vector is  0.21526835640255285\n",
      "Loss decreases to  1.342484652862956\n",
      "The norm of grad vector is  0.2143643284568424\n",
      "Loss decreases to  1.3420260970973532\n",
      "The norm of grad vector is  0.2134640970121153\n",
      "Loss decreases to  1.341571384690618\n",
      "The norm of grad vector is  0.21256764612481513\n",
      "Loss decreases to  1.3411204834298553\n",
      "The norm of grad vector is  0.2116749599183419\n",
      "Loss decreases to  1.3406733613721664\n",
      "The norm of grad vector is  0.2107860225827706\n",
      "Loss decreases to  1.3402299868423737\n",
      "The norm of grad vector is  0.2099008183745699\n",
      "Loss decreases to  1.3397903284307906\n",
      "The norm of grad vector is  0.20901933161632436\n",
      "Loss decreases to  1.339354354990984\n",
      "The norm of grad vector is  0.20814154669645712\n",
      "Loss decreases to  1.3389220356375728\n",
      "The norm of grad vector is  0.20726744806895112\n",
      "Loss decreases to  1.3384933397440426\n",
      "The norm of grad vector is  0.20639702025307652\n",
      "Loss decreases to  1.3380682369405739\n",
      "The norm of grad vector is  0.2055302478331158\n",
      "Loss decreases to  1.3376466971118857\n",
      "The norm of grad vector is  0.20466711545808852\n",
      "Loss decreases to  1.3372286903951096\n",
      "The norm of grad vector is  0.20380760784148327\n",
      "Loss decreases to  1.3368141871776695\n",
      "The norm of grad vector is  0.20295170976098487\n",
      "Loss decreases to  1.33640315809519\n",
      "The norm of grad vector is  0.20209940605820312\n",
      "Loss decreases to  1.335995574029405\n",
      "The norm of grad vector is  0.2012506816384078\n",
      "Loss decreases to  1.3355914061061052\n",
      "The norm of grad vector is  0.2004055214702587\n",
      "Loss decreases to  1.335190625693088\n",
      "The norm of grad vector is  0.1995639105855402\n",
      "Loss decreases to  1.3347932043981316\n",
      "The norm of grad vector is  0.198725834078898\n",
      "Loss decreases to  1.3343991140669793\n",
      "The norm of grad vector is  0.19789127710757073\n",
      "Loss decreases to  1.3340083267813485\n",
      "The norm of grad vector is  0.19706022489113115\n",
      "Loss decreases to  1.3336208148569533\n",
      "The norm of grad vector is  0.19623266271122447\n",
      "Loss decreases to  1.3332365508415407\n",
      "The norm of grad vector is  0.19540857591130287\n",
      "Loss decreases to  1.332855507512948\n",
      "The norm of grad vector is  0.194587949896373\n",
      "Loss decreases to  1.3324776578771753\n",
      "The norm of grad vector is  0.19377077013272978\n",
      "Loss decreases to  1.332102975166465\n",
      "The norm of grad vector is  0.19295702214770624\n",
      "Loss decreases to  1.3317314328374192\n",
      "The norm of grad vector is  0.19214669152941402\n",
      "Loss decreases to  1.3313630045691087\n",
      "The norm of grad vector is  0.19133976392648505\n",
      "Loss decreases to  1.3309976642612114\n",
      "The norm of grad vector is  0.19053622504782283\n",
      "Loss decreases to  1.330635386032162\n",
      "The norm of grad vector is  0.18973606066234672\n",
      "Loss decreases to  1.3302761442173274\n",
      "The norm of grad vector is  0.1889392565987381\n",
      "Loss decreases to  1.3299199133671715\n",
      "The norm of grad vector is  0.1881457987451948\n",
      "Loss decreases to  1.3295666682454674\n",
      "The norm of grad vector is  0.18735567304917533\n",
      "Loss decreases to  1.3292163838275046\n",
      "The norm of grad vector is  0.18656886551715196\n",
      "Loss decreases to  1.3288690352983117\n",
      "The norm of grad vector is  0.18578536221436479\n",
      "Loss decreases to  1.3285245980509093\n",
      "The norm of grad vector is  0.18500514926457298\n",
      "Loss decreases to  1.3281830476845515\n",
      "The norm of grad vector is  0.18422821284980953\n",
      "Loss decreases to  1.327844360003013\n",
      "The norm of grad vector is  0.1834545392101359\n",
      "Loss decreases to  1.3275085110128653\n",
      "The norm of grad vector is  0.18268411464339968\n",
      "Loss decreases to  1.327175476921777\n",
      "The norm of grad vector is  0.18191692550499178\n",
      "Loss decreases to  1.326845234136835\n",
      "The norm of grad vector is  0.18115295820760316\n",
      "Loss decreases to  1.3265177592628659\n",
      "The norm of grad vector is  0.18039219922098504\n",
      "Loss decreases to  1.3261930291007826\n",
      "The norm of grad vector is  0.17963463507170996\n",
      "Loss decreases to  1.3258710206459405\n",
      "The norm of grad vector is  0.1788802523429333\n",
      "Loss decreases to  1.3255517110865036\n",
      "The norm of grad vector is  0.1781290376741535\n",
      "Loss decreases to  1.3252350778018396\n",
      "The norm of grad vector is  0.17738097776097747\n",
      "Loss decreases to  1.3249210983609008\n",
      "The norm of grad vector is  0.17663605935488608\n",
      "Loss decreases to  1.3246097505206504\n",
      "The norm of grad vector is  0.17589426926299598\n",
      "Loss decreases to  1.3243010122244778\n",
      "The norm of grad vector is  0.1751555943478276\n",
      "Loss decreases to  1.323994861600636\n",
      "The norm of grad vector is  0.17442002152707425\n",
      "Loss decreases to  1.3236912769607023\n",
      "The norm of grad vector is  0.17368753777336887\n",
      "Loss decreases to  1.3233902367980255\n",
      "The norm of grad vector is  0.1729581301140509\n",
      "Loss decreases to  1.3230917197862186\n",
      "The norm of grad vector is  0.1722317856309418\n",
      "Loss decreases to  1.3227957047776346\n",
      "The norm of grad vector is  0.17150849146011307\n",
      "Loss decreases to  1.32250217080188\n",
      "The norm of grad vector is  0.1707882347916566\n",
      "Loss decreases to  1.3222110970643157\n",
      "The norm of grad vector is  0.170071002869463\n",
      "Loss decreases to  1.3219224629446005\n",
      "The norm of grad vector is  0.16935678299098916\n",
      "Loss decreases to  1.3216362479952168\n",
      "The norm of grad vector is  0.168645562507041\n",
      "Loss decreases to  1.3213524319400243\n",
      "The norm of grad vector is  0.16793732882154053\n",
      "Loss decreases to  1.321070994672832\n",
      "The norm of grad vector is  0.16723206939131344\n",
      "Loss decreases to  1.3207919162559618\n",
      "The norm of grad vector is  0.16652977172585373\n",
      "Loss decreases to  1.3205151769188477\n",
      "The norm of grad vector is  0.16583042338711634\n",
      "Loss decreases to  1.3202407570566241\n",
      "The norm of grad vector is  0.16513401198928618\n",
      "Loss decreases to  1.3199686372287476\n",
      "The norm of grad vector is  0.16444052519856425\n",
      "Loss decreases to  1.3196987981576118\n",
      "The norm of grad vector is  0.16374995073294857\n",
      "Loss decreases to  1.3194312207271826\n",
      "The norm of grad vector is  0.16306227636201318\n",
      "Loss decreases to  1.3191658859816497\n",
      "The norm of grad vector is  0.16237748990669848\n",
      "Loss decreases to  1.3189027751240783\n",
      "The norm of grad vector is  0.16169557923908776\n",
      "Loss decreases to  1.31864186951508\n",
      "The norm of grad vector is  0.16101653228219687\n",
      "Loss decreases to  1.318383150671487\n",
      "The norm of grad vector is  0.16034033700976097\n",
      "Loss decreases to  1.318126600265054\n",
      "The norm of grad vector is  0.1596669814460185\n",
      "Loss decreases to  1.3178722001211463\n",
      "The norm of grad vector is  0.15899645366550008\n",
      "Loss decreases to  1.3176199322174627\n",
      "The norm of grad vector is  0.15832874179281956\n",
      "Loss decreases to  1.3173697786827554\n",
      "The norm of grad vector is  0.15766383400246045\n",
      "Loss decreases to  1.3171217217955595\n",
      "The norm of grad vector is  0.1570017185185692\n",
      "Loss decreases to  1.3168757439829475\n",
      "The norm of grad vector is  0.15634238361474398\n",
      "Loss decreases to  1.3166318278192723\n",
      "The norm of grad vector is  0.1556858176138291\n",
      "Loss decreases to  1.3163899560249426\n",
      "The norm of grad vector is  0.1550320088877081\n",
      "Loss decreases to  1.316150111465193\n",
      "The norm of grad vector is  0.15438094585709738\n",
      "Loss decreases to  1.315912277148873\n",
      "The norm of grad vector is  0.15373261699133967\n",
      "Loss decreases to  1.315676436227243\n",
      "The norm of grad vector is  0.15308701080820175\n",
      "Loss decreases to  1.3154425719927796\n",
      "The norm of grad vector is  0.15244411587367113\n",
      "Loss decreases to  1.3152106678779925\n",
      "The norm of grad vector is  0.1518039208017531\n",
      "Loss decreases to  1.3149807074542528\n",
      "The norm of grad vector is  0.15116641425426788\n",
      "Loss decreases to  1.3147526744306233\n",
      "The norm of grad vector is  0.15053158494064964\n",
      "Loss decreases to  1.3145265526527097\n",
      "The norm of grad vector is  0.1498994216177502\n",
      "Loss decreases to  1.3143023261015179\n",
      "The norm of grad vector is  0.14926991308963763\n",
      "Loss decreases to  1.314079978892313\n",
      "The norm of grad vector is  0.14864304820739566\n",
      "Loss decreases to  1.3138594952734963\n",
      "The norm of grad vector is  0.14801881586892868\n",
      "Loss decreases to  1.3136408596254934\n",
      "The norm of grad vector is  0.14739720501876644\n",
      "Loss decreases to  1.3134240564596436\n",
      "The norm of grad vector is  0.14677820464786476\n",
      "Loss decreases to  1.3132090704170998\n",
      "The norm of grad vector is  0.14616180379341343\n",
      "Loss decreases to  1.3129958862677515\n",
      "The norm of grad vector is  0.14554799153864104\n",
      "Loss decreases to  1.3127844889091298\n",
      "The norm of grad vector is  0.1449367570126203\n",
      "Loss decreases to  1.3125748633653531\n",
      "The norm of grad vector is  0.14432808939007855\n",
      "Loss decreases to  1.3123669947860557\n",
      "The norm of grad vector is  0.14372197789120256\n",
      "Loss decreases to  1.312160868445342\n",
      "The norm of grad vector is  0.14311841178145146\n",
      "Loss decreases to  1.311956469740737\n",
      "The norm of grad vector is  0.14251738037136316\n",
      "Loss decreases to  1.3117537841921605\n",
      "The norm of grad vector is  0.14191887301636527\n",
      "Loss decreases to  1.3115527974408903\n",
      "The norm of grad vector is  0.14132287911659047\n",
      "Loss decreases to  1.3113534952485564\n",
      "The norm of grad vector is  0.140729388116684\n",
      "Loss decreases to  1.3111558634961253\n",
      "The norm of grad vector is  0.14013838950561885\n",
      "Loss decreases to  1.310959888182898\n",
      "The norm of grad vector is  0.13954987281651007\n",
      "Loss decreases to  1.3107655554255255\n",
      "The norm of grad vector is  0.13896382762642845\n",
      "Loss decreases to  1.31057285145702\n",
      "The norm of grad vector is  0.1383802435562172\n",
      "Loss decreases to  1.3103817626257817\n",
      "The norm of grad vector is  0.13779911027030556\n",
      "Loss decreases to  1.310192275394629\n",
      "The norm of grad vector is  0.1372204174765285\n",
      "Loss decreases to  1.3100043763398432\n",
      "The norm of grad vector is  0.13664415492594303\n",
      "Loss decreases to  1.3098180521502165\n",
      "The norm of grad vector is  0.13607031241264747\n",
      "Loss decreases to  1.3096332896261034\n",
      "The norm of grad vector is  0.13549887977359962\n",
      "Loss decreases to  1.3094500756784966\n",
      "The norm of grad vector is  0.13492984688843823\n",
      "Loss decreases to  1.3092683973280903\n",
      "The norm of grad vector is  0.13436320367930127\n",
      "Loss decreases to  1.309088241704365\n",
      "The norm of grad vector is  0.1337989401106504\n",
      "Loss decreases to  1.308909596044674\n",
      "The norm of grad vector is  0.13323704618909285\n",
      "Loss decreases to  1.3087324476933402\n",
      "The norm of grad vector is  0.13267751196319996\n",
      "Loss decreases to  1.3085567841007577\n",
      "The norm of grad vector is  0.13212032752333824\n",
      "Loss decreases to  1.3083825928225081\n",
      "The norm of grad vector is  0.13156548300148946\n",
      "Loss decreases to  1.3082098615184734\n",
      "The norm of grad vector is  0.13101296857107367\n",
      "Loss decreases to  1.3080385779519645\n",
      "The norm of grad vector is  0.13046277444678162\n",
      "Loss decreases to  1.3078687299888478\n",
      "The norm of grad vector is  0.1299148908843957\n",
      "Loss decreases to  1.3077003055966998\n",
      "The norm of grad vector is  0.12936930818062034\n",
      "Loss decreases to  1.3075332928439412\n",
      "The norm of grad vector is  0.12882601667290902\n",
      "Loss decreases to  1.307367679898996\n",
      "The norm of grad vector is  0.12828500673929377\n",
      "Loss decreases to  1.3072034550294578\n",
      "The norm of grad vector is  0.127746268798214\n",
      "Loss decreases to  1.3070406066012494\n",
      "The norm of grad vector is  0.1272097933083477\n",
      "Loss decreases to  1.3068791230778098\n",
      "The norm of grad vector is  0.1266755707684411\n",
      "Loss decreases to  1.306718993019264\n",
      "The norm of grad vector is  0.12614359171714387\n",
      "Loss decreases to  1.3065602050816256\n",
      "The norm of grad vector is  0.12561384673283504\n",
      "Loss decreases to  1.3064027480159857\n",
      "The norm of grad vector is  0.1250863264334634\n",
      "Loss decreases to  1.3062466106677162\n",
      "The norm of grad vector is  0.12456102147637814\n",
      "Loss decreases to  1.306091781975681\n",
      "The norm of grad vector is  0.12403792255815871\n",
      "Loss decreases to  1.3059382509714554\n",
      "The norm of grad vector is  0.1235170204144609\n",
      "Loss decreases to  1.30578600677854\n",
      "The norm of grad vector is  0.12299830581984275\n",
      "Loss decreases to  1.3056350386116016\n",
      "The norm of grad vector is  0.1224817695876047\n",
      "Loss decreases to  1.305485335775701\n",
      "The norm of grad vector is  0.1219674025696298\n",
      "Loss decreases to  1.305336887665541\n",
      "The norm of grad vector is  0.12145519565621575\n",
      "Loss decreases to  1.305189683764704\n",
      "The norm of grad vector is  0.12094513977591889\n",
      "Loss decreases to  1.3050437136449267\n",
      "The norm of grad vector is  0.12043722589539084\n",
      "Loss decreases to  1.3048989669653417\n",
      "The norm of grad vector is  0.11993144501921968\n",
      "Loss decreases to  1.3047554334717537\n",
      "The norm of grad vector is  0.11942778818976926\n",
      "Loss decreases to  1.304613102995917\n",
      "The norm of grad vector is  0.11892624648702095\n",
      "Loss decreases to  1.3044719654548038\n",
      "The norm of grad vector is  0.11842681102841719\n",
      "Loss decreases to  1.3043320108499026\n",
      "The norm of grad vector is  0.117929472968704\n",
      "Loss decreases to  1.3041932292665008\n",
      "The norm of grad vector is  0.11743422349976983\n",
      "Loss decreases to  1.3040556108729844\n",
      "The norm of grad vector is  0.1169410538504975\n",
      "Loss decreases to  1.3039191459201447\n",
      "The norm of grad vector is  0.1164499552866009\n",
      "Loss decreases to  1.3037838247404872\n",
      "The norm of grad vector is  0.11596091911047623\n",
      "Loss decreases to  1.3036496377475393\n",
      "The norm of grad vector is  0.11547393666104393\n",
      "Loss decreases to  1.303516575435183\n",
      "The norm of grad vector is  0.11498899931359839\n",
      "Loss decreases to  1.3033846283769694\n",
      "The norm of grad vector is  0.11450609847965394\n",
      "Loss decreases to  1.3032537872254617\n",
      "The norm of grad vector is  0.11402522560679056\n",
      "Loss decreases to  1.3031240427115651\n",
      "The norm of grad vector is  0.11354637217850587\n",
      "Loss decreases to  1.302995385643876\n",
      "The norm of grad vector is  0.11306952971406388\n",
      "Loss decreases to  1.3028678069080257\n",
      "The norm of grad vector is  0.11259468976834172\n",
      "Loss decreases to  1.3027412974660368\n",
      "The norm of grad vector is  0.11212184393168431\n",
      "Loss decreases to  1.3026158483556856\n",
      "The norm of grad vector is  0.11165098382975029\n",
      "Loss decreases to  1.3024914506898597\n",
      "The norm of grad vector is  0.11118210112336989\n",
      "Loss decreases to  1.302368095655942\n",
      "The norm of grad vector is  0.11071518750839139\n",
      "Loss decreases to  1.3022457745151708\n",
      "The norm of grad vector is  0.11025023471553896\n",
      "Loss decreases to  1.3021244786020278\n",
      "The norm of grad vector is  0.1097872345102621\n",
      "Loss decreases to  1.3020041993236309\n",
      "The norm of grad vector is  0.10932617869259295\n",
      "Loss decreases to  1.3018849281591098\n",
      "The norm of grad vector is  0.10886705909699813\n",
      "Loss decreases to  1.301766656659018\n",
      "The norm of grad vector is  0.10840986759223852\n",
      "Loss decreases to  1.301649376444727\n",
      "The norm of grad vector is  0.1079545960812181\n",
      "Loss decreases to  1.301533079207827\n",
      "The norm of grad vector is  0.1075012365008492\n",
      "Loss decreases to  1.3014177567095544\n",
      "The norm of grad vector is  0.10704978082190438\n",
      "Loss decreases to  1.3013034007801894\n",
      "The norm of grad vector is  0.10660022104887318\n",
      "Loss decreases to  1.3011900033184922\n",
      "The norm of grad vector is  0.10615254921982352\n",
      "Loss decreases to  1.3010775562911205\n",
      "The norm of grad vector is  0.10570675740626265\n",
      "Loss decreases to  1.300966051732063\n",
      "The norm of grad vector is  0.10526283771298935\n",
      "Loss decreases to  1.3008554817420777\n",
      "The norm of grad vector is  0.10482078227796117\n",
      "Loss decreases to  1.3007458384881252\n",
      "The norm of grad vector is  0.10438058327215236\n",
      "Loss decreases to  1.3006371142028195\n",
      "The norm of grad vector is  0.1039422328994153\n",
      "Loss decreases to  1.3005293011838803\n",
      "The norm of grad vector is  0.10350572339634248\n",
      "Loss decreases to  1.3004223917935813\n",
      "The norm of grad vector is  0.10307104703212903\n",
      "Loss decreases to  1.3003163784582075\n",
      "The norm of grad vector is  0.10263819610843626\n",
      "Loss decreases to  1.3002112536675279\n",
      "The norm of grad vector is  0.10220716295925417\n",
      "Loss decreases to  1.3001070099742564\n",
      "The norm of grad vector is  0.10177793995076907\n",
      "Loss decreases to  1.3000036399935238\n",
      "The norm of grad vector is  0.10135051948122095\n",
      "Loss decreases to  1.2999011364023605\n",
      "The norm of grad vector is  0.1009248939807784\n",
      "Loss decreases to  1.299799491939172\n",
      "The norm of grad vector is  0.10050105591139794\n",
      "Loss decreases to  1.2996986994032245\n",
      "The norm of grad vector is  0.10007899776669105\n",
      "Loss decreases to  1.2995987516541403\n",
      "The norm of grad vector is  0.09965871207179536\n",
      "Loss decreases to  1.2994996416113855\n",
      "The norm of grad vector is  0.09924019138323699\n",
      "Loss decreases to  1.2994013622537741\n",
      "The norm of grad vector is  0.09882342828880257\n",
      "Loss decreases to  1.2993039066189664\n",
      "The norm of grad vector is  0.09840841540740677\n",
      "Loss decreases to  1.2992072678029745\n",
      "The norm of grad vector is  0.09799514538896102\n",
      "Loss decreases to  1.2991114389596792\n",
      "The norm of grad vector is  0.09758361091424243\n",
      "Loss decreases to  1.2990164133003432\n",
      "The norm of grad vector is  0.097173804694768\n",
      "Loss decreases to  1.2989221840931207\n",
      "The norm of grad vector is  0.09676571947266183\n",
      "Loss decreases to  1.2988287446626015\n",
      "The norm of grad vector is  0.09635934802052672\n",
      "Loss decreases to  1.2987360883893109\n",
      "The norm of grad vector is  0.09595468314131807\n",
      "Loss decreases to  1.298644208709265\n",
      "The norm of grad vector is  0.09555171766821671\n",
      "Loss decreases to  1.2985530991134908\n",
      "The norm of grad vector is  0.0951504444644998\n",
      "Loss decreases to  1.2984627531475708\n",
      "The norm of grad vector is  0.09475085642341341\n",
      "Loss decreases to  1.2983731644111867\n",
      "The norm of grad vector is  0.0943529464680526\n",
      "Loss decreases to  1.298284326557662\n",
      "The norm of grad vector is  0.09395670755122976\n",
      "Loss decreases to  1.2981962332935135\n",
      "The norm of grad vector is  0.09356213265535218\n",
      "Loss decreases to  1.2981088783780093\n",
      "The norm of grad vector is  0.09316921479229866\n",
      "Loss decreases to  1.2980222556227234\n",
      "The norm of grad vector is  0.09277794700329497\n",
      "Loss decreases to  1.297936358891094\n",
      "The norm of grad vector is  0.09238832235878928\n",
      "Loss decreases to  1.2978511820979979\n",
      "The norm of grad vector is  0.09200033395833244\n",
      "Loss decreases to  1.2977667192093125\n",
      "The norm of grad vector is  0.09161397493045223\n",
      "Loss decreases to  1.2976829642414858\n",
      "The norm of grad vector is  0.0912292384325349\n",
      "Loss decreases to  1.2975999112611214\n",
      "The norm of grad vector is  0.09084611765070223\n",
      "Loss decreases to  1.297517554384553\n",
      "The norm of grad vector is  0.09046460579968969\n",
      "Loss decreases to  1.297435887777424\n",
      "The norm of grad vector is  0.09008469612273018\n",
      "Loss decreases to  1.2973549056542801\n",
      "The norm of grad vector is  0.08970638189142952\n",
      "Loss decreases to  1.2972746022781625\n",
      "The norm of grad vector is  0.08932965640565137\n",
      "Loss decreases to  1.2971949719601865\n",
      "The norm of grad vector is  0.0889545129933964\n",
      "Loss decreases to  1.2971160090591576\n",
      "The norm of grad vector is  0.0885809450106843\n",
      "Loss decreases to  1.297037707981158\n",
      "The norm of grad vector is  0.08820894584143557\n",
      "Loss decreases to  1.2969600631791554\n",
      "The norm of grad vector is  0.08783850889735914\n",
      "Loss decreases to  1.2968830691526096\n",
      "The norm of grad vector is  0.08746962761782529\n",
      "Loss decreases to  1.296806720447085\n",
      "The norm of grad vector is  0.087102295469763\n",
      "Loss decreases to  1.2967310116538602\n",
      "The norm of grad vector is  0.08673650594753038\n",
      "Loss decreases to  1.2966559374095437\n",
      "The norm of grad vector is  0.08637225257281127\n",
      "Loss decreases to  1.2965814923957037\n",
      "The norm of grad vector is  0.08600952889449374\n",
      "Loss decreases to  1.2965076713384824\n",
      "The norm of grad vector is  0.08564832848855662\n",
      "Loss decreases to  1.2964344690082188\n",
      "The norm of grad vector is  0.08528864495795863\n",
      "Loss decreases to  1.2963618802190922\n",
      "The norm of grad vector is  0.08493047193252033\n",
      "Loss decreases to  1.2962898998287384\n",
      "The norm of grad vector is  0.08457380306881691\n",
      "Loss decreases to  1.2962185227379033\n",
      "The norm of grad vector is  0.08421863205006229\n",
      "Loss decreases to  1.296147743890062\n",
      "The norm of grad vector is  0.08386495258599613\n",
      "Loss decreases to  1.2960775582710773\n",
      "The norm of grad vector is  0.0835127584127771\n",
      "Loss decreases to  1.2960079609088357\n",
      "The norm of grad vector is  0.0831620432928673\n",
      "Loss decreases to  1.295938946872897\n",
      "The norm of grad vector is  0.08281280101492387\n",
      "Loss decreases to  1.2958705112741438\n",
      "The norm of grad vector is  0.08246502539369036\n",
      "Loss decreases to  1.295802649264439\n",
      "The norm of grad vector is  0.08211871026988135\n",
      "Loss decreases to  1.2957353560362792\n",
      "The norm of grad vector is  0.0817738495100826\n",
      "Loss decreases to  1.2956686268224549\n",
      "The norm of grad vector is  0.08143043700663345\n",
      "Loss decreases to  1.2956024568957105\n",
      "The norm of grad vector is  0.08108846667752723\n",
      "Loss decreases to  1.2955368415684143\n",
      "The norm of grad vector is  0.08074793246629192\n",
      "Loss decreases to  1.295471776192223\n",
      "The norm of grad vector is  0.08040882834189737\n",
      "Loss decreases to  1.2954072561577543\n",
      "The norm of grad vector is  0.08007114829863554\n",
      "Loss decreases to  1.295343276894254\n",
      "The norm of grad vector is  0.07973488635602187\n",
      "Loss decreases to  1.2952798338692857\n",
      "The norm of grad vector is  0.07940003655868805\n",
      "Loss decreases to  1.2952169225883923\n",
      "The norm of grad vector is  0.07906659297627287\n",
      "Loss decreases to  1.295154538594795\n",
      "The norm of grad vector is  0.07873454970332226\n",
      "Loss decreases to  1.2950926774690659\n",
      "The norm of grad vector is  0.07840390085918092\n",
      "Loss decreases to  1.2950313348288138\n",
      "The norm of grad vector is  0.07807464058788975\n",
      "Loss decreases to  1.2949705063283845\n",
      "The norm of grad vector is  0.07774676305808367\n",
      "Loss decreases to  1.294910187658548\n",
      "The norm of grad vector is  0.07742026246288462\n",
      "Loss decreases to  1.2948503745461846\n",
      "The norm of grad vector is  0.07709513301980052\n",
      "Loss decreases to  1.294791062753994\n",
      "The norm of grad vector is  0.07677136897062602\n",
      "Loss decreases to  1.2947322480801877\n",
      "The norm of grad vector is  0.07644896458133363\n",
      "Loss decreases to  1.2946739263582017\n",
      "The norm of grad vector is  0.07612791414197972\n",
      "Loss decreases to  1.294616093456383\n",
      "The norm of grad vector is  0.07580821196659644\n",
      "Loss decreases to  1.294558745277718\n",
      "The norm of grad vector is  0.07548985239309808\n",
      "Loss decreases to  1.2945018777595254\n",
      "The norm of grad vector is  0.07517282978317316\n",
      "Loss decreases to  1.2944454868731754\n",
      "The norm of grad vector is  0.07485713852219163\n",
      "Loss decreases to  1.2943895686238092\n",
      "The norm of grad vector is  0.07454277301910066\n",
      "Loss decreases to  1.2943341190500437\n",
      "The norm of grad vector is  0.07422972770632955\n",
      "Loss decreases to  1.2942791342237017\n",
      "The norm of grad vector is  0.07391799703968364\n",
      "Loss decreases to  1.2942246102495316\n",
      "The norm of grad vector is  0.07360757549825697\n",
      "Loss decreases to  1.294170543264925\n",
      "The norm of grad vector is  0.07329845758432657\n",
      "Loss decreases to  1.2941169294396504\n",
      "The norm of grad vector is  0.0729906378232574\n",
      "Loss decreases to  1.294063764975582\n",
      "The norm of grad vector is  0.07268411076340427\n",
      "Loss decreases to  1.2940110461064194\n",
      "The norm of grad vector is  0.07237887097601857\n",
      "Loss decreases to  1.2939587690974372\n",
      "The norm of grad vector is  0.07207491305514845\n",
      "Loss decreases to  1.2939069302452095\n",
      "The norm of grad vector is  0.07177223161754462\n",
      "Loss decreases to  1.2938555258773508\n",
      "The norm of grad vector is  0.07147082130256588\n",
      "Loss decreases to  1.2938045523522548\n",
      "The norm of grad vector is  0.07117067677208277\n",
      "Loss decreases to  1.293754006058837\n",
      "The norm of grad vector is  0.07087179271038203\n",
      "Loss decreases to  1.2937038834162815\n",
      "The norm of grad vector is  0.07057416382407715\n",
      "Loss decreases to  1.2936541808737803\n",
      "The norm of grad vector is  0.07027778484200889\n",
      "Loss decreases to  1.293604894910291\n",
      "The norm of grad vector is  0.06998265051515534\n",
      "Loss decreases to  1.2935560220342803\n",
      "The norm of grad vector is  0.06968875561653716\n",
      "Loss decreases to  1.2935075587834777\n",
      "The norm of grad vector is  0.06939609494112764\n",
      "Loss decreases to  1.2934595017246338\n",
      "The norm of grad vector is  0.06910466330575694\n",
      "Loss decreases to  1.2934118474532748\n",
      "The norm of grad vector is  0.06881445554902366\n",
      "Loss decreases to  1.2933645925934585\n",
      "The norm of grad vector is  0.06852546653120015\n",
      "Loss decreases to  1.2933177337975421\n",
      "The norm of grad vector is  0.06823769113414554\n",
      "Loss decreases to  1.2932712677459384\n",
      "The norm of grad vector is  0.06795112426121164\n",
      "Loss decreases to  1.293225191146882\n",
      "The norm of grad vector is  0.0676657608371524\n",
      "Loss decreases to  1.2931795007361992\n",
      "The norm of grad vector is  0.06738159580803815\n",
      "Loss decreases to  1.293134193277076\n",
      "The norm of grad vector is  0.06709862414116165\n",
      "Loss decreases to  1.293089265559824\n",
      "The norm of grad vector is  0.0668168408249509\n",
      "Loss decreases to  1.293044714401658\n",
      "The norm of grad vector is  0.06653624086888042\n",
      "Loss decreases to  1.2930005366464719\n",
      "The norm of grad vector is  0.06625681930338331\n",
      "Loss decreases to  1.292956729164608\n",
      "The norm of grad vector is  0.06597857117976218\n",
      "Loss decreases to  1.2929132888526416\n",
      "The norm of grad vector is  0.0657014915701004\n",
      "Loss decreases to  1.2928702126331617\n",
      "The norm of grad vector is  0.06542557556717753\n",
      "Loss decreases to  1.292827497454546\n",
      "The norm of grad vector is  0.06515081828438353\n",
      "Loss decreases to  1.2927851402907495\n",
      "The norm of grad vector is  0.06487721485562485\n",
      "Loss decreases to  1.2927431381410959\n",
      "The norm of grad vector is  0.06460476043524654\n",
      "Loss decreases to  1.2927014880300516\n",
      "The norm of grad vector is  0.06433345019794443\n",
      "Loss decreases to  1.2926601870070245\n",
      "The norm of grad vector is  0.0640632793386741\n",
      "Loss decreases to  1.292619232146157\n",
      "The norm of grad vector is  0.06379424307257424\n",
      "Loss decreases to  1.292578620546106\n",
      "The norm of grad vector is  0.06352633663487535\n",
      "Loss decreases to  1.2925383493298543\n",
      "The norm of grad vector is  0.06325955528081881\n",
      "Loss decreases to  1.292498415644494\n",
      "The norm of grad vector is  0.0629938942855723\n",
      "Loss decreases to  1.2924588166610274\n",
      "The norm of grad vector is  0.06272934894414378\n",
      "Loss decreases to  1.2924195495741708\n",
      "The norm of grad vector is  0.06246591457130102\n",
      "Loss decreases to  1.2923806116021535\n",
      "The norm of grad vector is  0.06220358650148883\n",
      "Loss decreases to  1.2923419999865147\n",
      "The norm of grad vector is  0.06194236008874316\n",
      "Loss decreases to  1.2923037119919214\n",
      "The norm of grad vector is  0.0616822307066117\n",
      "Loss decreases to  1.2922657449059587\n",
      "The norm of grad vector is  0.06142319374807083\n",
      "Loss decreases to  1.292228096038951\n",
      "The norm of grad vector is  0.06116524462544596\n",
      "Loss decreases to  1.2921907627237648\n",
      "The norm of grad vector is  0.0609083787703255\n",
      "Loss decreases to  1.2921537423156186\n",
      "The norm of grad vector is  0.06065259163348636\n",
      "Loss decreases to  1.292117032191903\n",
      "The norm of grad vector is  0.06039787868480864\n",
      "Loss decreases to  1.292080629751985\n",
      "The norm of grad vector is  0.060144235413194705\n",
      "Loss decreases to  1.2920445324170298\n",
      "The norm of grad vector is  0.059891657326495866\n",
      "Loss decreases to  1.2920087376298186\n",
      "The norm of grad vector is  0.059640139951426765\n",
      "Loss decreases to  1.2919732428545663\n",
      "The norm of grad vector is  0.059389678833483915\n",
      "Loss decreases to  1.291938045576736\n",
      "The norm of grad vector is  0.05914026953687652\n",
      "Loss decreases to  1.2919031433028731\n",
      "The norm of grad vector is  0.058891907644438184\n",
      "Loss decreases to  1.2918685335604185\n",
      "The norm of grad vector is  0.05864458875755428\n",
      "Loss decreases to  1.2918342138975367\n",
      "The norm of grad vector is  0.05839830849608297\n",
      "Loss decreases to  1.2918001818829414\n",
      "The norm of grad vector is  0.05815306249827335\n",
      "Loss decreases to  1.2917664351057256\n",
      "The norm of grad vector is  0.05790884642069671\n",
      "Loss decreases to  1.2917329711751895\n",
      "The norm of grad vector is  0.05766565593816111\n",
      "Loss decreases to  1.2916997877206704\n",
      "The norm of grad vector is  0.05742348674364034\n",
      "Loss decreases to  1.2916668823913715\n",
      "The norm of grad vector is  0.057182334548195324\n",
      "Loss decreases to  1.291634252856204\n",
      "The norm of grad vector is  0.05694219508089764\n",
      "Loss decreases to  1.2916018968036165\n",
      "The norm of grad vector is  0.056703064088756035\n",
      "Loss decreases to  1.2915698119414267\n",
      "The norm of grad vector is  0.05646493733663935\n",
      "Loss decreases to  1.2915379959966726\n",
      "The norm of grad vector is  0.05622781060720158\n",
      "Loss decreases to  1.2915064467154362\n",
      "The norm of grad vector is  0.055991679700809184\n",
      "Loss decreases to  1.2914751618626903\n",
      "The norm of grad vector is  0.055756540435462555\n",
      "Loss decreases to  1.2914441392221458\n",
      "The norm of grad vector is  0.05552238864672705\n",
      "Loss decreases to  1.2914133765960851\n",
      "The norm of grad vector is  0.0552892201876555\n",
      "Loss decreases to  1.2913828718052105\n",
      "The norm of grad vector is  0.05505703092871645\n",
      "Loss decreases to  1.2913526226884917\n",
      "The norm of grad vector is  0.05482581675772095\n",
      "Loss decreases to  1.291322627103011\n",
      "The norm of grad vector is  0.054595573579746336\n",
      "Loss decreases to  1.2912928829238124\n",
      "The norm of grad vector is  0.0543662973170717\n",
      "Loss decreases to  1.2912633880437454\n",
      "The norm of grad vector is  0.05413798390909628\n",
      "Loss decreases to  1.2912341403733254\n",
      "The norm of grad vector is  0.05391062931227491\n",
      "Loss decreases to  1.2912051378405787\n",
      "The norm of grad vector is  0.053684229500043264\n",
      "Loss decreases to  1.291176378390898\n",
      "The norm of grad vector is  0.05345878046274351\n",
      "Loss decreases to  1.2911478599868964\n",
      "The norm of grad vector is  0.05323427820756071\n",
      "Loss decreases to  1.2911195806082623\n",
      "The norm of grad vector is  0.05301071875844492\n",
      "Loss decreases to  1.2910915382516213\n",
      "The norm of grad vector is  0.05278809815604541\n",
      "Loss decreases to  1.291063730930384\n",
      "The norm of grad vector is  0.05256641245763936\n",
      "Loss decreases to  1.2910361566746165\n",
      "The norm of grad vector is  0.05234565773705882\n",
      "Loss decreases to  1.2910088135308924\n",
      "The norm of grad vector is  0.05212583008462609\n",
      "Loss decreases to  1.2909816995621592\n",
      "The norm of grad vector is  0.05190692560708343\n",
      "Loss decreases to  1.2909548128476023\n",
      "The norm of grad vector is  0.05168894042751994\n",
      "Loss decreases to  1.290928151482502\n",
      "The norm of grad vector is  0.051471870685308875\n",
      "Loss decreases to  1.2909017135781056\n",
      "The norm of grad vector is  0.051255712536033365\n",
      "Loss decreases to  1.2908754972614889\n",
      "The norm of grad vector is  0.051040462151424\n",
      "Loss decreases to  1.290849500675427\n",
      "The norm of grad vector is  0.05082611571928745\n",
      "Loss decreases to  1.2908237219782615\n",
      "The norm of grad vector is  0.050612669443438056\n",
      "Loss decreases to  1.290798159343769\n",
      "The norm of grad vector is  0.050400119543637094\n",
      "Loss decreases to  1.290772810961031\n",
      "The norm of grad vector is  0.050188462255514724\n",
      "Loss decreases to  1.2907476750343123\n",
      "The norm of grad vector is  0.04997769383051549\n",
      "Loss decreases to  1.2907227497829206\n",
      "The norm of grad vector is  0.049767810535822306\n",
      "Loss decreases to  1.2906980334410956\n",
      "The norm of grad vector is  0.04955880865429622\n",
      "Loss decreases to  1.2906735242578717\n",
      "The norm of grad vector is  0.04935068448440743\n",
      "Loss decreases to  1.2906492204969635\n",
      "The norm of grad vector is  0.04914343434017194\n",
      "Loss decreases to  1.290625120436635\n",
      "The norm of grad vector is  0.04893705455108368\n",
      "Loss decreases to  1.2906012223695784\n",
      "The norm of grad vector is  0.04873154146205238\n",
      "Loss decreases to  1.2905775246028033\n",
      "The norm of grad vector is  0.04852689143333782\n",
      "Loss decreases to  1.2905540254575019\n",
      "The norm of grad vector is  0.048323100840481865\n",
      "Loss decreases to  1.2905307232689398\n",
      "The norm of grad vector is  0.04812016607425269\n",
      "Loss decreases to  1.2905076163863334\n",
      "The norm of grad vector is  0.047918083540571504\n",
      "Loss decreases to  1.2904847031727416\n",
      "The norm of grad vector is  0.04771684966045273\n",
      "Loss decreases to  1.2904619820049361\n",
      "The norm of grad vector is  0.04751646086994451\n",
      "Loss decreases to  1.2904394512732944\n",
      "The norm of grad vector is  0.04731691362005781\n",
      "Loss decreases to  1.2904171093816905\n",
      "The norm of grad vector is  0.04711820437671102\n",
      "Loss decreases to  1.29039495474737\n",
      "The norm of grad vector is  0.046920329620661894\n",
      "Loss decreases to  1.2903729858008457\n",
      "The norm of grad vector is  0.046723285847448705\n",
      "Loss decreases to  1.2903512009857883\n",
      "The norm of grad vector is  0.046527069567325385\n",
      "Loss decreases to  1.2903295987589096\n",
      "The norm of grad vector is  0.046331677305203385\n",
      "Loss decreases to  1.290308177589858\n",
      "The norm of grad vector is  0.046137105600585315\n",
      "Loss decreases to  1.2902869359611076\n",
      "The norm of grad vector is  0.045943351007507585\n",
      "Loss decreases to  1.2902658723678493\n",
      "The norm of grad vector is  0.0457504100944799\n",
      "Loss decreases to  1.2902449853178932\n",
      "The norm of grad vector is  0.04555827944441932\n",
      "Loss decreases to  1.2902242733315459\n",
      "The norm of grad vector is  0.04536695565459559\n",
      "Loss decreases to  1.2902037349415263\n",
      "The norm of grad vector is  0.04517643533656533\n",
      "Loss decreases to  1.2901833686928459\n",
      "The norm of grad vector is  0.04498671511611828\n",
      "Loss decreases to  1.2901631731427095\n",
      "The norm of grad vector is  0.04479779163321465\n",
      "Loss decreases to  1.2901431468604223\n",
      "The norm of grad vector is  0.04460966154192112\n",
      "Loss decreases to  1.2901232884272693\n",
      "The norm of grad vector is  0.04442232151036032\n",
      "Loss decreases to  1.2901035964364382\n",
      "The norm of grad vector is  0.04423576822064399\n",
      "Loss decreases to  1.290084069492897\n",
      "The norm of grad vector is  0.044049998368819246\n",
      "Loss decreases to  1.2900647062133141\n",
      "The norm of grad vector is  0.043865008664808074\n",
      "Loss decreases to  1.2900455052259507\n",
      "The norm of grad vector is  0.04368079583234846\n",
      "Loss decreases to  1.2900264651705617\n",
      "The norm of grad vector is  0.04349735660893768\n",
      "Loss decreases to  1.2900075846983041\n",
      "The norm of grad vector is  0.04331468774577557\n",
      "Loss decreases to  1.2899888624716453\n",
      "The norm of grad vector is  0.04313278600770128\n",
      "Loss decreases to  1.2899702971642555\n",
      "The norm of grad vector is  0.042951648173145084\n",
      "Loss decreases to  1.2899518874609226\n",
      "The norm of grad vector is  0.042771271034062947\n",
      "Loss decreases to  1.2899336320574664\n",
      "The norm of grad vector is  0.042591651395884236\n",
      "Loss decreases to  1.289915529660626\n",
      "The norm of grad vector is  0.042412786077453954\n",
      "Loss decreases to  1.2898975789879892\n",
      "The norm of grad vector is  0.04223467191097716\n",
      "Loss decreases to  1.2898797787678837\n",
      "The norm of grad vector is  0.042057305741961946\n",
      "Loss decreases to  1.289862127739303\n",
      "The norm of grad vector is  0.041880684429162394\n",
      "Loss decreases to  1.2898446246518092\n",
      "The norm of grad vector is  0.04170480484452572\n",
      "Loss decreases to  1.2898272682654364\n",
      "The norm of grad vector is  0.04152966387313603\n",
      "Loss decreases to  1.2898100573506235\n",
      "The norm of grad vector is  0.04135525841315657\n",
      "Loss decreases to  1.289792990688106\n",
      "The norm of grad vector is  0.04118158537578048\n",
      "Loss decreases to  1.2897760670688418\n",
      "The norm of grad vector is  0.04100864168516781\n",
      "Loss decreases to  1.2897592852939226\n",
      "The norm of grad vector is  0.04083642427839913\n",
      "Loss decreases to  1.2897426441744866\n",
      "The norm of grad vector is  0.04066493010541641\n",
      "Loss decreases to  1.2897261425316442\n",
      "The norm of grad vector is  0.04049415612897125\n",
      "Loss decreases to  1.2897097791963712\n",
      "The norm of grad vector is  0.04032409932456972\n",
      "Loss decreases to  1.289693553009455\n",
      "The norm of grad vector is  0.040154756680420095\n",
      "Loss decreases to  1.289677462821395\n",
      "The norm of grad vector is  0.039986125197377625\n",
      "Loss decreases to  1.2896615074923246\n",
      "The norm of grad vector is  0.03981820188889304\n",
      "Loss decreases to  1.2896456858919307\n",
      "The norm of grad vector is  0.03965098378096062\n",
      "Loss decreases to  1.2896299968993736\n",
      "The norm of grad vector is  0.03948446791206082\n",
      "Loss decreases to  1.2896144394032059\n",
      "The norm of grad vector is  0.03931865133311539\n",
      "Loss decreases to  1.289599012301303\n",
      "The norm of grad vector is  0.03915353110742611\n",
      "Loss decreases to  1.2895837145007691\n",
      "The norm of grad vector is  0.03898910431063263\n",
      "Loss decreases to  1.2895685449178746\n",
      "The norm of grad vector is  0.03882536803065089\n",
      "Loss decreases to  1.2895535024779692\n",
      "The norm of grad vector is  0.03866231936763067\n",
      "Loss decreases to  1.2895385861154114\n",
      "The norm of grad vector is  0.03849995543389449\n",
      "Loss decreases to  1.2895237947734917\n",
      "The norm of grad vector is  0.038338273353897286\n",
      "Loss decreases to  1.2895091274043562\n",
      "The norm of grad vector is  0.03817727026416575\n",
      "Loss decreases to  1.2894945829689357\n",
      "The norm of grad vector is  0.038016943313254295\n",
      "Loss decreases to  1.2894801604368644\n",
      "The norm of grad vector is  0.037857289661691315\n",
      "Loss decreases to  1.289465858786421\n",
      "The norm of grad vector is  0.03769830648192893\n",
      "Loss decreases to  1.2894516770044409\n",
      "The norm of grad vector is  0.037539990958295674\n",
      "Loss decreases to  1.2894376140862531\n",
      "The norm of grad vector is  0.03738234028694263\n",
      "Loss decreases to  1.2894236690356078\n",
      "The norm of grad vector is  0.03722535167579563\n",
      "Loss decreases to  1.2894098408646064\n",
      "The norm of grad vector is  0.037069022344507346\n",
      "Loss decreases to  1.2893961285936262\n",
      "The norm of grad vector is  0.03691334952440715\n",
      "Loss decreases to  1.2893825312512606\n",
      "The norm of grad vector is  0.036758330458449236\n",
      "Loss decreases to  1.2893690478742392\n",
      "The norm of grad vector is  0.03660396240116789\n",
      "Loss decreases to  1.289355677507371\n",
      "The norm of grad vector is  0.03645024261862579\n",
      "Loss decreases to  1.2893424192034662\n",
      "The norm of grad vector is  0.03629716838836788\n",
      "Loss decreases to  1.2893292720232745\n",
      "The norm of grad vector is  0.03614473699937183\n",
      "Loss decreases to  1.289316235035421\n",
      "The norm of grad vector is  0.03599294575200182\n",
      "Loss decreases to  1.2893033073163327\n",
      "The norm of grad vector is  0.03584179195795608\n",
      "Loss decreases to  1.2892904879501825\n",
      "The norm of grad vector is  0.035691272940224845\n",
      "Loss decreases to  1.2892777760288134\n",
      "The norm of grad vector is  0.0355413860330399\n",
      "Loss decreases to  1.289265170651685\n",
      "The norm of grad vector is  0.035392128581828185\n",
      "Loss decreases to  1.2892526709258034\n",
      "The norm of grad vector is  0.03524349794316514\n",
      "Loss decreases to  1.2892402759656578\n",
      "The norm of grad vector is  0.0350954914847259\n",
      "Loss decreases to  1.2892279848931598\n",
      "The norm of grad vector is  0.03494810658524089\n",
      "Loss decreases to  1.28921579683758\n",
      "The norm of grad vector is  0.03480134063444947\n",
      "Loss decreases to  1.2892037109354906\n",
      "The norm of grad vector is  0.03465519103305194\n",
      "Loss decreases to  1.2891917263306965\n",
      "The norm of grad vector is  0.034509655192665065\n",
      "Loss decreases to  1.289179842174183\n",
      "The norm of grad vector is  0.03436473053577451\n",
      "Loss decreases to  1.2891680576240445\n",
      "The norm of grad vector is  0.03422041449569137\n",
      "Loss decreases to  1.2891563718454364\n",
      "The norm of grad vector is  0.03407670451650575\n",
      "Loss decreases to  1.2891447840105124\n",
      "The norm of grad vector is  0.03393359805304115\n",
      "Loss decreases to  1.2891332932983626\n",
      "The norm of grad vector is  0.033791092570809755\n",
      "Loss decreases to  1.2891218988949593\n",
      "The norm of grad vector is  0.03364918554596549\n",
      "Loss decreases to  1.289110599993093\n",
      "The norm of grad vector is  0.03350787446526486\n",
      "Loss decreases to  1.2890993957923262\n",
      "The norm of grad vector is  0.03336715682601602\n",
      "Loss decreases to  1.2890882854989263\n",
      "The norm of grad vector is  0.03322703013603845\n",
      "Loss decreases to  1.289077268325813\n",
      "The norm of grad vector is  0.033087491913617456\n",
      "Loss decreases to  1.2890663434925052\n",
      "The norm of grad vector is  0.03294853968745969\n",
      "Loss decreases to  1.2890555102250603\n",
      "The norm of grad vector is  0.03281017099665195\n",
      "Loss decreases to  1.2890447677560277\n",
      "The norm of grad vector is  0.032672383390613395\n",
      "Loss decreases to  1.289034115324383\n",
      "The norm of grad vector is  0.03253517442905674\n",
      "Loss decreases to  1.2890235521754816\n",
      "The norm of grad vector is  0.03239854168194041\n",
      "Loss decreases to  1.2890130775610098\n",
      "The norm of grad vector is  0.03226248272942983\n",
      "Loss decreases to  1.2890026907389176\n",
      "The norm of grad vector is  0.03212699516185303\n",
      "Loss decreases to  1.2889923909733803\n",
      "The norm of grad vector is  0.031992076579654305\n",
      "Loss decreases to  1.2889821775347348\n",
      "The norm of grad vector is  0.03185772459335885\n",
      "Loss decreases to  1.2889720496994435\n",
      "The norm of grad vector is  0.031723936823523445\n",
      "Loss decreases to  1.2889620067500205\n",
      "The norm of grad vector is  0.03159071090069989\n",
      "Loss decreases to  1.2889520479750038\n",
      "The norm of grad vector is  0.0314580444653897\n",
      "Loss decreases to  1.2889421726688886\n",
      "The norm of grad vector is  0.03132593516800204\n",
      "Loss decreases to  1.288932380132086\n",
      "The norm of grad vector is  0.03119438066881455\n",
      "Loss decreases to  1.2889226696708678\n",
      "The norm of grad vector is  0.031063378637931006\n",
      "Loss decreases to  1.2889130405973246\n",
      "The norm of grad vector is  0.03093292675523843\n",
      "Loss decreases to  1.2889034922293106\n",
      "The norm of grad vector is  0.030803022710367507\n",
      "Loss decreases to  1.288894023890395\n",
      "The norm of grad vector is  0.030673664202652814\n",
      "Loss decreases to  1.288884634909819\n",
      "The norm of grad vector is  0.030544848941087943\n",
      "Loss decreases to  1.2888753246224456\n",
      "The norm of grad vector is  0.0304165746442907\n",
      "Loss decreases to  1.2888660923687134\n",
      "The norm of grad vector is  0.03028883904045804\n",
      "Loss decreases to  1.288856937494586\n",
      "The norm of grad vector is  0.030161639867326374\n",
      "Loss decreases to  1.2888478593515102\n",
      "The norm of grad vector is  0.030034974872136178\n",
      "Loss decreases to  1.2888388572963698\n",
      "The norm of grad vector is  0.029908841811584066\n",
      "Loss decreases to  1.2888299306914404\n",
      "The norm of grad vector is  0.02978323845179008\n",
      "Loss decreases to  1.2888210789043375\n",
      "The norm of grad vector is  0.029658162568254716\n",
      "Loss decreases to  1.2888123013079822\n",
      "The norm of grad vector is  0.02953361194582151\n",
      "Loss decreases to  1.2888035972805456\n",
      "The norm of grad vector is  0.02940958437863514\n",
      "Loss decreases to  1.2887949662054181\n",
      "The norm of grad vector is  0.02928607767010476\n",
      "Loss decreases to  1.2887864074711515\n",
      "The norm of grad vector is  0.029163089632864094\n",
      "Loss decreases to  1.2887779204714243\n",
      "The norm of grad vector is  0.02904061808873195\n",
      "Loss decreases to  1.2887695046049996\n",
      "The norm of grad vector is  0.028918660868675988\n",
      "Loss decreases to  1.2887611592756785\n",
      "The norm of grad vector is  0.028797215812771594\n",
      "Loss decreases to  1.288752883892255\n",
      "The norm of grad vector is  0.028676280770164816\n",
      "Loss decreases to  1.2887446778684843\n",
      "The norm of grad vector is  0.028555853599035452\n",
      "Loss decreases to  1.2887365406230304\n",
      "The norm of grad vector is  0.0284359321665567\n",
      "Loss decreases to  1.2887284715794338\n",
      "The norm of grad vector is  0.028316514348859418\n",
      "Loss decreases to  1.2887204701660615\n",
      "The norm of grad vector is  0.02819759803099281\n",
      "Loss decreases to  1.2887125358160805\n",
      "The norm of grad vector is  0.028079181106889926\n",
      "Loss decreases to  1.2887046679673955\n",
      "The norm of grad vector is  0.02796126147932603\n",
      "Loss decreases to  1.2886968660626357\n",
      "The norm of grad vector is  0.02784383705988502\n",
      "Loss decreases to  1.2886891295490923\n",
      "The norm of grad vector is  0.027726905768920337\n",
      "Loss decreases to  1.2886814578786936\n",
      "The norm of grad vector is  0.02761046553552022\n",
      "Loss decreases to  1.2886738505079613\n",
      "The norm of grad vector is  0.02749451429746873\n",
      "Loss decreases to  1.2886663068979705\n",
      "The norm of grad vector is  0.02737905000121135\n",
      "Loss decreases to  1.2886588265143148\n",
      "The norm of grad vector is  0.027264070601814988\n",
      "Loss decreases to  1.288651408827066\n",
      "The norm of grad vector is  0.027149574062938105\n",
      "Loss decreases to  1.2886440533107368\n",
      "The norm of grad vector is  0.027035558356787316\n",
      "Loss decreases to  1.2886367594442467\n",
      "The norm of grad vector is  0.026922021464088564\n",
      "Loss decreases to  1.28862952671088\n",
      "The norm of grad vector is  0.02680896137404418\n",
      "Loss decreases to  1.2886223545982547\n",
      "The norm of grad vector is  0.026696376084302857\n",
      "Loss decreases to  1.28861524259828\n",
      "The norm of grad vector is  0.026584263600923495\n",
      "Loss decreases to  1.2886081902071242\n",
      "The norm of grad vector is  0.026472621938335706\n",
      "Loss decreases to  1.2886011969251818\n",
      "The norm of grad vector is  0.02636144911930889\n",
      "Loss decreases to  1.2885942622570299\n",
      "The norm of grad vector is  0.02625074317491651\n",
      "Loss decreases to  1.288587385711403\n",
      "The norm of grad vector is  0.026140502144501056\n",
      "Loss decreases to  1.2885805668011474\n",
      "The norm of grad vector is  0.026030724075636207\n",
      "Loss decreases to  1.2885738050431979\n",
      "The norm of grad vector is  0.025921407024097957\n",
      "Loss decreases to  1.2885670999585361\n",
      "The norm of grad vector is  0.025812549053825126\n",
      "Loss decreases to  1.288560451072157\n",
      "The norm of grad vector is  0.02570414823688824\n",
      "Loss decreases to  1.2885538579130387\n",
      "The norm of grad vector is  0.025596202653453397\n",
      "Loss decreases to  1.2885473200141027\n",
      "The norm of grad vector is  0.02548871039175092\n",
      "Loss decreases to  1.288540836912195\n",
      "The norm of grad vector is  0.02538166954803652\n",
      "Loss decreases to  1.2885344081480334\n",
      "The norm of grad vector is  0.025275078226564333\n",
      "Loss decreases to  1.2885280332661901\n",
      "The norm of grad vector is  0.02516893453954668\n",
      "Loss decreases to  1.2885217118150527\n",
      "The norm of grad vector is  0.02506323660712497\n",
      "Loss decreases to  1.2885154433467954\n",
      "The norm of grad vector is  0.02495798255733674\n",
      "Loss decreases to  1.2885092274173475\n",
      "The norm of grad vector is  0.024853170526076964\n",
      "Loss decreases to  1.2885030635863541\n",
      "The norm of grad vector is  0.02474879865707354\n",
      "Loss decreases to  1.2884969514171576\n",
      "The norm of grad vector is  0.024644865101845867\n",
      "Loss decreases to  1.288490890476756\n",
      "The norm of grad vector is  0.024541368019680376\n",
      "Loss decreases to  1.288484880335781\n",
      "The norm of grad vector is  0.024438305577589765\n",
      "Loss decreases to  1.2884789205684568\n",
      "The norm of grad vector is  0.024335675950285678\n",
      "Loss decreases to  1.2884730107525801\n",
      "The norm of grad vector is  0.024233477320145216\n",
      "Loss decreases to  1.288467150469487\n",
      "The norm of grad vector is  0.024131707877180435\n",
      "Loss decreases to  1.2884613393040205\n",
      "The norm of grad vector is  0.024030365819001392\n",
      "Loss decreases to  1.2884555768445072\n",
      "The norm of grad vector is  0.023929449350788553\n",
      "Loss decreases to  1.2884498626827154\n",
      "The norm of grad vector is  0.02382895668526149\n",
      "Loss decreases to  1.2884441964138418\n",
      "The norm of grad vector is  0.02372888604264309\n",
      "Loss decreases to  1.2884385776364782\n",
      "The norm of grad vector is  0.0236292356506326\n",
      "Loss decreases to  1.2884330059525737\n",
      "The norm of grad vector is  0.023530003744369102\n",
      "Loss decreases to  1.2884274809674199\n",
      "The norm of grad vector is  0.023431188566406577\n",
      "Loss decreases to  1.2884220022896127\n",
      "The norm of grad vector is  0.023332788366677567\n",
      "Loss decreases to  1.2884165695310281\n",
      "The norm of grad vector is  0.023234801402465786\n",
      "Loss decreases to  1.2884111823067972\n",
      "The norm of grad vector is  0.023137225938371454\n",
      "Loss decreases to  1.2884058402352792\n",
      "The norm of grad vector is  0.023040060246282926\n",
      "Loss decreases to  1.2884005429380274\n",
      "The norm of grad vector is  0.02294330260534829\n",
      "Loss decreases to  1.2883952900397677\n",
      "The norm of grad vector is  0.022846951301939134\n",
      "Loss decreases to  1.2883900811683762\n",
      "The norm of grad vector is  0.022751004629625496\n",
      "Loss decreases to  1.288384915954839\n",
      "The norm of grad vector is  0.022655460889143164\n",
      "Loss decreases to  1.2883797940332435\n",
      "The norm of grad vector is  0.02256031838836389\n",
      "Loss decreases to  1.2883747150407403\n",
      "The norm of grad vector is  0.02246557544226658\n",
      "Loss decreases to  1.2883696786175225\n",
      "The norm of grad vector is  0.02237123037290431\n",
      "Loss decreases to  1.288364684406797\n",
      "The norm of grad vector is  0.022277281509378844\n",
      "Loss decreases to  1.2883597320547635\n",
      "The norm of grad vector is  0.02218372718780761\n",
      "Loss decreases to  1.2883548212105824\n",
      "The norm of grad vector is  0.02209056575129715\n",
      "Loss decreases to  1.2883499515263612\n",
      "The norm of grad vector is  0.021997795549909965\n",
      "Loss decreases to  1.288345122657121\n",
      "The norm of grad vector is  0.021905414940639443\n",
      "Loss decreases to  1.2883403342607693\n",
      "The norm of grad vector is  0.021813422287377784\n",
      "Loss decreases to  1.2883355859980865\n",
      "The norm of grad vector is  0.021721815960889138\n",
      "Loss decreases to  1.2883308775326974\n",
      "The norm of grad vector is  0.021630594338777617\n",
      "Loss decreases to  1.28832620853104\n",
      "The norm of grad vector is  0.02153975580546353\n",
      "Loss decreases to  1.2883215786623523\n",
      "The norm of grad vector is  0.021449298752149456\n",
      "Loss decreases to  1.2883169875986455\n",
      "The norm of grad vector is  0.021359221576795703\n",
      "Loss decreases to  1.2883124350146737\n",
      "The norm of grad vector is  0.021269522684089884\n",
      "Loss decreases to  1.2883079205879233\n",
      "The norm of grad vector is  0.021180200485417865\n",
      "Loss decreases to  1.2883034439985839\n",
      "The norm of grad vector is  0.021091253398839518\n",
      "Loss decreases to  1.2882990049295202\n",
      "The norm of grad vector is  0.021002679849055936\n",
      "Loss decreases to  1.28829460306626\n",
      "The norm of grad vector is  0.020914478267384955\n",
      "Loss decreases to  1.2882902380969636\n",
      "The norm of grad vector is  0.020826647091732386\n",
      "Loss decreases to  1.2882859097124078\n",
      "The norm of grad vector is  0.020739184766562505\n",
      "Loss decreases to  1.2882816176059597\n",
      "The norm of grad vector is  0.02065208974287476\n",
      "Loss decreases to  1.2882773614735559\n",
      "The norm of grad vector is  0.02056536047817051\n",
      "Loss decreases to  1.2882731410136807\n",
      "The norm of grad vector is  0.02047899543643103\n",
      "Loss decreases to  1.2882689559273468\n",
      "The norm of grad vector is  0.020392993088087636\n",
      "Loss decreases to  1.2882648059180755\n",
      "The norm of grad vector is  0.020307351909995896\n",
      "Loss decreases to  1.2882606906918714\n",
      "The norm of grad vector is  0.020222070385405005\n",
      "Loss decreases to  1.2882566099571973\n",
      "The norm of grad vector is  0.02013714700393847\n",
      "Loss decreases to  1.2882525634249664\n",
      "The norm of grad vector is  0.02005258026155924\n",
      "Loss decreases to  1.2882485508085146\n",
      "The norm of grad vector is  0.019968368660546014\n",
      "Loss decreases to  1.2882445718235769\n",
      "The norm of grad vector is  0.019884510709469654\n",
      "Loss decreases to  1.2882406261882733\n",
      "The norm of grad vector is  0.019801004923164223\n",
      "Loss decreases to  1.2882367136230872\n",
      "The norm of grad vector is  0.019717849822698814\n",
      "Loss decreases to  1.288232833850842\n",
      "The norm of grad vector is  0.019635043935354726\n",
      "Loss decreases to  1.2882289865966836\n",
      "The norm of grad vector is  0.019552585794597094\n",
      "Loss decreases to  1.2882251715880668\n",
      "The norm of grad vector is  0.019470473940052076\n",
      "Loss decreases to  1.2882213885547256\n",
      "The norm of grad vector is  0.019388706917474956\n",
      "Loss decreases to  1.288217637228662\n",
      "The norm of grad vector is  0.01930728327873197\n",
      "Loss decreases to  1.2882139173441212\n",
      "The norm of grad vector is  0.01922620158176789\n",
      "Loss decreases to  1.2882102286375805\n",
      "The norm of grad vector is  0.01914546039058576\n",
      "Loss decreases to  1.288206570847724\n",
      "The norm of grad vector is  0.019065058275217713\n",
      "Loss decreases to  1.288202943715421\n",
      "The norm of grad vector is  0.01898499381170178\n",
      "Loss decreases to  1.2881993469837205\n",
      "The norm of grad vector is  0.018905265582054596\n",
      "Loss decreases to  1.288195780397821\n",
      "The norm of grad vector is  0.01882587217425009\n",
      "Loss decreases to  1.2881922437050557\n",
      "The norm of grad vector is  0.018746812182190927\n",
      "Loss decreases to  1.2881887366548788\n",
      "The norm of grad vector is  0.0186680842056835\n",
      "Loss decreases to  1.288185258998842\n",
      "The norm of grad vector is  0.018589686850415014\n",
      "Loss decreases to  1.28818181049058\n",
      "The norm of grad vector is  0.018511618727930324\n",
      "Loss decreases to  1.2881783908857922\n",
      "The norm of grad vector is  0.018433878455602473\n",
      "Loss decreases to  1.2881749999422256\n",
      "The norm of grad vector is  0.018356464656610307\n",
      "Loss decreases to  1.2881716374196572\n",
      "The norm of grad vector is  0.018279375959919065\n",
      "Loss decreases to  1.2881683030798827\n",
      "The norm of grad vector is  0.01820261100024706\n",
      "Loss decreases to  1.2881649966866813\n",
      "The norm of grad vector is  0.018126168418048424\n",
      "Loss decreases to  1.2881617180058278\n",
      "The norm of grad vector is  0.01805004685948604\n",
      "Loss decreases to  1.2881584668050494\n",
      "The norm of grad vector is  0.017974244976407563\n",
      "Loss decreases to  1.2881552428540237\n",
      "The norm of grad vector is  0.017898761426325557\n",
      "Loss decreases to  1.2881520459243576\n",
      "The norm of grad vector is  0.017823594872386293\n",
      "Loss decreases to  1.2881488757895774\n",
      "The norm of grad vector is  0.01774874398335177\n",
      "Loss decreases to  1.2881457322250984\n",
      "The norm of grad vector is  0.017674207433575874\n",
      "Loss decreases to  1.288142615008224\n",
      "The norm of grad vector is  0.017599983902977897\n",
      "Loss decreases to  1.288139523918125\n",
      "The norm of grad vector is  0.017526072077021736\n",
      "Loss decreases to  1.2881364587358197\n",
      "The norm of grad vector is  0.017452470646690445\n",
      "Loss decreases to  1.2881334192441658\n",
      "The norm of grad vector is  0.017379178308467358\n",
      "Loss decreases to  1.2881304052278355\n",
      "The norm of grad vector is  0.017306193764306036\n",
      "Loss decreases to  1.288127416473312\n",
      "The norm of grad vector is  0.017233515721614103\n",
      "Loss decreases to  1.2881244527688613\n",
      "The norm of grad vector is  0.017161142893226962\n",
      "Loss decreases to  1.2881215139045301\n",
      "The norm of grad vector is  0.01708907399738427\n",
      "Loss decreases to  1.2881185996721227\n",
      "The norm of grad vector is  0.01701730775770945\n",
      "Loss decreases to  1.2881157098651859\n",
      "The norm of grad vector is  0.016945842903186507\n",
      "Loss decreases to  1.2881128442790004\n",
      "The norm of grad vector is  0.016874678168136387\n",
      "Loss decreases to  1.2881100027105628\n",
      "The norm of grad vector is  0.016803812292195037\n",
      "Loss decreases to  1.2881071849585664\n",
      "The norm of grad vector is  0.016733244020292498\n",
      "Loss decreases to  1.2881043908233982\n",
      "The norm of grad vector is  0.016662972102628765\n",
      "Loss decreases to  1.2881016201071143\n",
      "The norm of grad vector is  0.016592995294652493\n",
      "Loss decreases to  1.2880988726134275\n",
      "The norm of grad vector is  0.016523312357038737\n",
      "Loss decreases to  1.288096148147704\n",
      "The norm of grad vector is  0.016453922055667803\n",
      "Loss decreases to  1.2880934465169323\n",
      "The norm of grad vector is  0.01638482316160118\n",
      "Loss decreases to  1.2880907675297264\n",
      "The norm of grad vector is  0.016316014451063503\n",
      "Loss decreases to  1.2880881109962963\n",
      "The norm of grad vector is  0.01624749470541765\n",
      "Loss decreases to  1.288085476728448\n",
      "The norm of grad vector is  0.016179262711143356\n",
      "Loss decreases to  1.2880828645395657\n",
      "The norm of grad vector is  0.016111317259817595\n",
      "Loss decreases to  1.2880802742445956\n",
      "The norm of grad vector is  0.016043657148092583\n",
      "Loss decreases to  1.288077705660033\n",
      "The norm of grad vector is  0.015976281177673364\n",
      "Loss decreases to  1.2880751586039154\n",
      "The norm of grad vector is  0.015909188155296742\n",
      "Loss decreases to  1.288072632895802\n",
      "The norm of grad vector is  0.015842376892711566\n",
      "Loss decreases to  1.2880701283567673\n",
      "The norm of grad vector is  0.01577584620665561\n",
      "Loss decreases to  1.2880676448093835\n",
      "The norm of grad vector is  0.015709594918837923\n",
      "Loss decreases to  1.288065182077708\n",
      "The norm of grad vector is  0.01564362185591415\n",
      "Loss decreases to  1.2880627399872786\n",
      "The norm of grad vector is  0.015577925849468287\n",
      "Loss decreases to  1.2880603183650878\n",
      "The norm of grad vector is  0.015512505735990337\n",
      "Loss decreases to  1.2880579170395872\n",
      "The norm of grad vector is  0.015447360356856142\n",
      "Loss decreases to  1.288055535840656\n",
      "The norm of grad vector is  0.015382488558309506\n",
      "Loss decreases to  1.2880531745996093\n",
      "The norm of grad vector is  0.015317889191437707\n",
      "Loss decreases to  1.2880508331491678\n",
      "The norm of grad vector is  0.015253561112151374\n",
      "Loss decreases to  1.2880485113234612\n",
      "The norm of grad vector is  0.015189503181169657\n",
      "Loss decreases to  1.2880462089580016\n",
      "The norm of grad vector is  0.015125714263993428\n",
      "Loss decreases to  1.2880439258896856\n",
      "The norm of grad vector is  0.015062193230888568\n",
      "Loss decreases to  1.288041661956779\n",
      "The norm of grad vector is  0.014998938956865087\n",
      "Loss decreases to  1.288039416998892\n",
      "The norm of grad vector is  0.014935950321658679\n",
      "Loss decreases to  1.288037190856993\n",
      "The norm of grad vector is  0.014873226209707492\n",
      "Loss decreases to  1.2880349833733755\n",
      "The norm of grad vector is  0.014810765510136345\n",
      "Loss decreases to  1.288032794391655\n",
      "The norm of grad vector is  0.014748567116733766\n",
      "Loss decreases to  1.2880306237567574\n",
      "The norm of grad vector is  0.014686629927936075\n",
      "Loss decreases to  1.2880284713149142\n",
      "The norm of grad vector is  0.014624952846803136\n",
      "Loss decreases to  1.2880263369136384\n",
      "The norm of grad vector is  0.014563534781003506\n",
      "Loss decreases to  1.2880242204017236\n",
      "The norm of grad vector is  0.014502374642790807\n",
      "Loss decreases to  1.288022121629234\n",
      "The norm of grad vector is  0.014441471348988913\n",
      "Loss decreases to  1.2880200404474849\n",
      "The norm of grad vector is  0.014380823820969469\n",
      "Loss decreases to  1.28801797670904\n",
      "The norm of grad vector is  0.014320430984633021\n",
      "Loss decreases to  1.2880159302677012\n",
      "The norm of grad vector is  0.01426029177039374\n",
      "Loss decreases to  1.2880139009784943\n",
      "The norm of grad vector is  0.014200405113154223\n",
      "Loss decreases to  1.288011888697656\n",
      "The norm of grad vector is  0.014140769952292008\n",
      "Loss decreases to  1.2880098932826356\n",
      "The norm of grad vector is  0.014081385231637336\n",
      "Loss decreases to  1.2880079145920729\n",
      "The norm of grad vector is  0.014022249899456772\n",
      "Loss decreases to  1.2880059524857908\n",
      "The norm of grad vector is  0.01396336290843498\n",
      "Loss decreases to  1.2880040068247895\n",
      "The norm of grad vector is  0.013904723215652143\n",
      "Loss decreases to  1.2880020774712362\n",
      "The norm of grad vector is  0.013846329782569533\n",
      "Loss decreases to  1.2880001642884498\n",
      "The norm of grad vector is  0.013788181575010576\n",
      "Loss decreases to  1.2879982671408916\n",
      "The norm of grad vector is  0.013730277563141421\n",
      "Loss decreases to  1.287996385894168\n",
      "The norm of grad vector is  0.013672616721450478\n",
      "Loss decreases to  1.2879945204150085\n",
      "The norm of grad vector is  0.013615198028737297\n",
      "Loss decreases to  1.2879926705712534\n",
      "The norm of grad vector is  0.013558020468086936\n",
      "Loss decreases to  1.2879908362318588\n",
      "The norm of grad vector is  0.013501083026855364\n",
      "Loss decreases to  1.2879890172668733\n",
      "The norm of grad vector is  0.013444384696651858\n",
      "Loss decreases to  1.2879872135474388\n",
      "The norm of grad vector is  0.013387924473320758\n",
      "Loss decreases to  1.287985424945778\n",
      "The norm of grad vector is  0.013331701356922453\n",
      "Loss decreases to  1.2879836513351757\n",
      "The norm of grad vector is  0.013275714351716632\n",
      "Loss decreases to  1.287981892589991\n",
      "The norm of grad vector is  0.013219962466145527\n",
      "Loss decreases to  1.2879801485856281\n",
      "The norm of grad vector is  0.013164444712814803\n",
      "Loss decreases to  1.2879784191985382\n",
      "The norm of grad vector is  0.01310916010847766\n",
      "Loss decreases to  1.2879767043062071\n",
      "The norm of grad vector is  0.013054107674014121\n",
      "Loss decreases to  1.287975003787146\n",
      "The norm of grad vector is  0.012999286434419232\n",
      "Loss decreases to  1.2879733175208905\n",
      "The norm of grad vector is  0.01294469541877954\n",
      "Loss decreases to  1.2879716453879775\n",
      "The norm of grad vector is  0.012890333660261377\n",
      "Loss decreases to  1.2879699872699506\n",
      "The norm of grad vector is  0.012836200196090651\n",
      "Loss decreases to  1.2879683430493447\n",
      "The norm of grad vector is  0.012782294067535671\n",
      "Loss decreases to  1.2879667126096797\n",
      "The norm of grad vector is  0.012728614319893033\n",
      "Loss decreases to  1.2879650958354523\n",
      "The norm of grad vector is  0.012675160002464747\n",
      "Loss decreases to  1.2879634926121237\n",
      "The norm of grad vector is  0.012621930168549064\n",
      "Loss decreases to  1.2879619028261213\n",
      "The norm of grad vector is  0.01256892387541878\n",
      "Loss decreases to  1.2879603263648196\n",
      "The norm of grad vector is  0.01251614018430473\n",
      "Loss decreases to  1.2879587631165355\n",
      "The norm of grad vector is  0.012463578160382715\n",
      "Loss decreases to  1.287957212970529\n",
      "The norm of grad vector is  0.012411236872750678\n",
      "Loss decreases to  1.2879556758169821\n",
      "The norm of grad vector is  0.012359115394417997\n",
      "Loss decreases to  1.287954151546999\n",
      "The norm of grad vector is  0.012307212802288577\n",
      "Loss decreases to  1.287952640052598\n",
      "The norm of grad vector is  0.012255528177139966\n",
      "Loss decreases to  1.287951141226699\n",
      "The norm of grad vector is  0.012204060603611181\n",
      "Loss decreases to  1.2879496549631269\n",
      "The norm of grad vector is  0.012152809170185231\n",
      "Loss decreases to  1.2879481811565856\n",
      "The norm of grad vector is  0.012101772969174028\n",
      "Loss decreases to  1.2879467197026693\n",
      "The norm of grad vector is  0.012050951096699934\n",
      "Loss decreases to  1.2879452704978456\n",
      "The norm of grad vector is  0.012000342652682213\n",
      "Loss decreases to  1.2879438334394473\n",
      "The norm of grad vector is  0.011949946740819397\n",
      "Loss decreases to  1.287942408425673\n",
      "The norm of grad vector is  0.011899762468574224\n",
      "Loss decreases to  1.287940995355571\n",
      "The norm of grad vector is  0.0118497889471581\n",
      "Loss decreases to  1.2879395941290346\n",
      "The norm of grad vector is  0.011800025291514193\n",
      "Loss decreases to  1.2879382046467978\n",
      "The norm of grad vector is  0.011750470620303622\n",
      "Loss decreases to  1.2879368268104248\n",
      "The norm of grad vector is  0.01170112405588763\n",
      "Loss decreases to  1.2879354605223117\n",
      "The norm of grad vector is  0.01165198472431244\n",
      "Loss decreases to  1.2879341056856624\n",
      "The norm of grad vector is  0.011603051755298138\n",
      "Loss decreases to  1.2879327622044994\n",
      "The norm of grad vector is  0.011554324282214413\n",
      "Loss decreases to  1.2879314299836446\n",
      "The norm of grad vector is  0.01150580144207489\n",
      "Loss decreases to  1.287930108928725\n",
      "The norm of grad vector is  0.011457482375514225\n",
      "Loss decreases to  1.2879287989461512\n",
      "The norm of grad vector is  0.01140936622677733\n",
      "Loss decreases to  1.2879274999431205\n",
      "The norm of grad vector is  0.01136145214370304\n",
      "Loss decreases to  1.28792621182761\n",
      "The norm of grad vector is  0.011313739277709808\n",
      "Loss decreases to  1.2879249345083645\n",
      "The norm of grad vector is  0.011266226783776527\n",
      "Loss decreases to  1.287923667894899\n",
      "The norm of grad vector is  0.01121891382043441\n",
      "Loss decreases to  1.287922411897481\n",
      "The norm of grad vector is  0.011171799549745589\n",
      "Loss decreases to  1.287921166427131\n",
      "The norm of grad vector is  0.011124883137294045\n",
      "Loss decreases to  1.2879199313956224\n",
      "The norm of grad vector is  0.011078163752164622\n",
      "Loss decreases to  1.287918706715458\n",
      "The norm of grad vector is  0.011031640566934444\n",
      "Loss decreases to  1.2879174922998826\n",
      "The norm of grad vector is  0.01098531275765419\n",
      "Loss decreases to  1.2879162880628585\n",
      "The norm of grad vector is  0.01093917950383388\n",
      "Loss decreases to  1.287915093919079\n",
      "The norm of grad vector is  0.01089323998843186\n",
      "Loss decreases to  1.2879139097839478\n",
      "The norm of grad vector is  0.010847493397834375\n",
      "Loss decreases to  1.287912735573578\n",
      "The norm of grad vector is  0.010801938921847509\n",
      "Loss decreases to  1.2879115712047835\n",
      "The norm of grad vector is  0.010756575753677389\n",
      "Loss decreases to  1.2879104165950808\n",
      "The norm of grad vector is  0.010711403089919328\n",
      "Loss decreases to  1.2879092716626714\n",
      "The norm of grad vector is  0.010666420130543069\n",
      "Loss decreases to  1.2879081363264488\n",
      "The norm of grad vector is  0.010621626078877311\n",
      "Loss decreases to  1.287907010505981\n",
      "The norm of grad vector is  0.010577020141596985\n",
      "Loss decreases to  1.287905894121513\n",
      "The norm of grad vector is  0.010532601528707882\n",
      "Loss decreases to  1.2879047870939573\n",
      "The norm of grad vector is  0.01048836945353294\n",
      "Loss decreases to  1.2879036893448899\n",
      "The norm of grad vector is  0.01044432313270184\n",
      "Loss decreases to  1.287902600796542\n",
      "The norm of grad vector is  0.010400461786130813\n",
      "Loss decreases to  1.2879015213718015\n",
      "The norm of grad vector is  0.010356784637013417\n",
      "Loss decreases to  1.2879004509941976\n",
      "The norm of grad vector is  0.010313290911804518\n",
      "Loss decreases to  1.287899389587902\n",
      "The norm of grad vector is  0.010269979840208446\n",
      "Loss decreases to  1.2878983370777208\n",
      "The norm of grad vector is  0.010226850655164155\n",
      "Loss decreases to  1.2878972933890953\n",
      "The norm of grad vector is  0.010183902592831902\n",
      "Loss decreases to  1.287896258448087\n",
      "The norm of grad vector is  0.010141134892580566\n",
      "Loss decreases to  1.2878952321813755\n",
      "The norm of grad vector is  0.01009854679697122\n",
      "Loss decreases to  1.2878942145162602\n",
      "The norm of grad vector is  0.010056137551747394\n",
      "Loss decreases to  1.2878932053806464\n",
      "The norm of grad vector is  0.010013906405821033\n",
      "Loss decreases to  1.2878922047030448\n",
      "The norm of grad vector is  0.009971852611255088\n",
      "Loss decreases to  1.287891212412566\n",
      "The norm of grad vector is  0.00992997542325722\n",
      "Loss decreases to  1.2878902284389118\n",
      "The norm of grad vector is  0.009888274100160488\n",
      "Loss decreases to  1.2878892527123773\n",
      "The norm of grad vector is  0.009846747903413465\n",
      "Loss decreases to  1.2878882851638382\n",
      "The norm of grad vector is  0.009805396097566586\n",
      "Loss decreases to  1.2878873257247523\n",
      "The norm of grad vector is  0.009764217950256882\n",
      "Loss decreases to  1.2878863743271498\n",
      "The norm of grad vector is  0.009723212732199537\n",
      "Loss decreases to  1.2878854309036312\n",
      "The norm of grad vector is  0.009682379717171966\n",
      "Loss decreases to  1.287884495387364\n",
      "The norm of grad vector is  0.00964171818199971\n",
      "Loss decreases to  1.2878835677120728\n",
      "The norm of grad vector is  0.009601227406546906\n",
      "Loss decreases to  1.2878826478120382\n",
      "The norm of grad vector is  0.009560906673701578\n",
      "Loss decreases to  1.2878817356220935\n",
      "The norm of grad vector is  0.00952075526936275\n",
      "Loss decreases to  1.287880831077616\n",
      "The norm of grad vector is  0.009480772482428406\n",
      "Loss decreases to  1.2878799341145253\n",
      "The norm of grad vector is  0.009440957604782868\n",
      "Loss decreases to  1.2878790446692812\n",
      "The norm of grad vector is  0.009401309931285365\n",
      "Loss decreases to  1.2878781626788705\n",
      "The norm of grad vector is  0.009361828759754828\n",
      "Loss decreases to  1.287877288080812\n",
      "The norm of grad vector is  0.00932251339096047\n",
      "Loss decreases to  1.2878764208131477\n",
      "The norm of grad vector is  0.009283363128606387\n",
      "Loss decreases to  1.2878755608144363\n",
      "The norm of grad vector is  0.009244377279322055\n",
      "Loss decreases to  1.2878747080237558\n",
      "The norm of grad vector is  0.009205555152647937\n",
      "Loss decreases to  1.2878738623806902\n",
      "The norm of grad vector is  0.009166896061024127\n",
      "Loss decreases to  1.2878730238253346\n",
      "The norm of grad vector is  0.009128399319779528\n",
      "Loss decreases to  1.2878721922982823\n",
      "The norm of grad vector is  0.009090064247117254\n",
      "Loss decreases to  1.2878713677406275\n",
      "The norm of grad vector is  0.009051890164102743\n",
      "Loss decreases to  1.2878705500939531\n",
      "The norm of grad vector is  0.00901387639465405\n",
      "Loss decreases to  1.2878697393003398\n",
      "The norm of grad vector is  0.008976022265528424\n",
      "Loss decreases to  1.2878689353023447\n",
      "The norm of grad vector is  0.008938327106309782\n",
      "Loss decreases to  1.2878681380430146\n",
      "The norm of grad vector is  0.008900790249397791\n",
      "Loss decreases to  1.2878673474658668\n",
      "The norm of grad vector is  0.00886341102999541\n",
      "Loss decreases to  1.2878665635148958\n",
      "The norm of grad vector is  0.008826188786097429\n",
      "Loss decreases to  1.2878657861345655\n",
      "The norm of grad vector is  0.008789122858480547\n",
      "Loss decreases to  1.2878650152698043\n",
      "The norm of grad vector is  0.008752212590686405\n",
      "Loss decreases to  1.2878642508660008\n",
      "The norm of grad vector is  0.008715457329017061\n",
      "Loss decreases to  1.2878634928690043\n",
      "The norm of grad vector is  0.00867885642251673\n",
      "Loss decreases to  1.2878627412251158\n",
      "The norm of grad vector is  0.008642409222965735\n",
      "Loss decreases to  1.2878619958810877\n",
      "The norm of grad vector is  0.008606115084866223\n",
      "Loss decreases to  1.2878612567841177\n",
      "The norm of grad vector is  0.008569973365429489\n",
      "Loss decreases to  1.2878605238818477\n",
      "The norm of grad vector is  0.008533983424567829\n",
      "Loss decreases to  1.287859797122356\n",
      "The norm of grad vector is  0.008498144624880006\n",
      "Loss decreases to  1.2878590764541569\n",
      "The norm of grad vector is  0.008462456331644868\n",
      "Loss decreases to  1.287858361826197\n",
      "The norm of grad vector is  0.008426917912803967\n",
      "Loss decreases to  1.2878576531878498\n",
      "The norm of grad vector is  0.008391528738952683\n",
      "Loss decreases to  1.2878569504889141\n",
      "The norm of grad vector is  0.008356288183331296\n",
      "Loss decreases to  1.28785625367961\n",
      "The norm of grad vector is  0.008321195621813065\n",
      "Loss decreases to  1.287855562710573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h = linear_regression_1D()\n",
    "\n",
    "# You may edit the learning rate if the current setting does not yield convergence\n",
    "h.set_learning_rate(.01)\n",
    "\n",
    "# Uncomment the following to fit the vector w to our data. \n",
    "# You may also edit the number of iterations if the current setting does not yield convergence\n",
    "h.fit(x, y, iteration=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQaZJREFUeJzt3Xl4VOXB/vHvZA8hCQTIRhII+x4SpIqAolAUC4JhEWJbrXbRN2HVWrBuuEFdEBDU+v5abd8SUBBQaNEqCqKiKEmAsK8SkhA2k8lCJsnM+f0RjYKgJEzmzGTuz3Xl0jlz5sztFZm5Oc9zzmMxDMNARERExEV8zA4gIiIi3kXlQ0RERFxK5UNERERcSuVDREREXErlQ0RERFxK5UNERERcSuVDREREXErlQ0RERFzKz+wA53M4HBQUFBAaGorFYjE7joiIiFwCwzAoLS0lNjYWH58fP7fhduWjoKCA+Ph4s2OIiIhIA+Tl5REXF/ej+7hd+QgNDQVqw4eFhZmcRkRERC6F1WolPj6+7nv8x7hd+fh2qCUsLEzlQ0RExMNcypSJek04femll+jTp09dMRgwYADr1q2re76yspL09HRatWpF8+bNGTt2LEVFRfVPLiIiIk1WvcpHXFwcc+fOZevWrXz55Zdcf/31jB49mp07dwIwffp01qxZw/Lly9m4cSMFBQWkpqY2SnARERHxTBbDMIzLOUBERATPPPMM48aNo02bNmRmZjJu3DgA9uzZQ/fu3dm8eTNXXXXVJR3ParUSHh5OSUmJhl1EREQ8RH2+vxt8nw+73c6yZcsoLy9nwIABbN26lerqaoYNG1a3T7du3UhISGDz5s0NfRsRERFpYuo94XTHjh0MGDCAyspKmjdvzqpVq+jRowc5OTkEBATQokWLc/aPiori+PHjFz2ezWbDZrPVPbZarfWNJCIiIh6k3mc+unbtSk5ODp9//jn33HMPt99+O7t27WpwgDlz5hAeHl73o3t8iIiING31Lh8BAQF06tSJfv36MWfOHJKSkliwYAHR0dFUVVVRXFx8zv5FRUVER0df9HizZs2ipKSk7icvL6/e/xEiIiLiOS57bReHw4HNZqNfv374+/uzfv36uuf27t3L0aNHGTBgwEVfHxgYWHfpru7tISIi0vTVa87HrFmzGDFiBAkJCZSWlpKZmcmGDRt49913CQ8P56677mLGjBlEREQQFhbG5MmTGTBgwCVf6SIiIiJNX73Kx4kTJ/j1r39NYWEh4eHh9OnTh3fffZef//znADz//PP4+PgwduxYbDYbN9xwAy+++GKjBBcRERHPdNn3+XA23edDRETE87jkPh8iIiIiDaHyISIi4iUqq+3MWrmd5V+ae2Wp261qKyIiIs534EQZGZlZ7Dleyts5Bfy8RxQtmgWYkkXlQ0REpIl7c+sxHlydy9lqO62bBzL/1r6mFQ9Q+RAREWmyKqpqePitnazYegyAqzu2Yv7EvkSGBpmaS+VDRESkCdpXVEr6kiz2nyjDxwLThnUh/bpO+PpYzI6m8iEiItKUGIbBG1/m8cjbO6msdhAZGsiCickM6NjK7Gh1VD5ERESaiDJbDQ+u2sHqnAIABnduzfO39qV180CTk51L5UNERKQJ2FVgJSMzi0OnyvH1sXDv8C7cfU1HfNxgmOV8Kh8iIiIezDAMMrccZfaaXVTVOIgJD2LhpGT6t48wO9pFqXyIiIh4qNLKamau3MG/txcCcH23SJ4dn0REiHmX0V4KlQ8REREPlJtfQnpmFl+drsDPx8KfbuzGXYMS3XKY5XwqHyIiIh7EMAz+ufkrnvz3bqrsDtq2COaFtGRSElqaHe2SqXyIiIh4iJKz1fxpxXbe2XkcgOE9onhmXBLhzfxNTlY/Kh8iIiIeICevmIzMLI59fRZ/XwsP3NSdO65uj8VSj2EWux02bYLCQoiJgcGDwde38UJfhMqHiIiIGzMMg799fJi/vLOHartBQkQzFqUl0yeuRf0OtHIlTJ0Kx459ty0uDhYsgNRUp2b+KSofIiIibqq4oor7lm/j/d0nALipdzRzx/YhLKiewywrV8K4cWAY527Pz6/dvmKFSwuIxTDOT2Iuq9VKeHg4JSUlhIWFmR1HRETEFFu/OsPkzGwKSioJ8PPhoZE9+OWVCfUbZoHaoZb27c894/F9FkvtGZDDhy9rCKY+39868yEiIuJGHA6DVzYd4pl392J3GCS2DmFRWjI9Y8MbdsBNmy5ePKD2bEheXu1+Q4Y07D3qSeVDRETETZwus3Hv8m1s2HsSgJuTYnkqtTfNAy/j67qw0Ln7OYHKh4iIiBv4/NBppizLpshqI9DPh9k39+TW/vH1H2Y5X0yMc/dzApUPEREREzkcBi9uOMC89/bhMKBjmxAW35ZCt2gnzXscPLh2Tkd+/g8nnMJ3cz4GD3bO+10CH5e9k4iIiJzjZKmN21/dwrP/rS0eqSlteTtjkPOKB9ROIl2woPbfzz+L8u3j+fNder8PlQ8RERETfHrgFDct3MSm/acI9vflmXF9mDehLyGXM7/jYlJTay+nbdv23O1xcS6/zBY07CIiIuJSdofBwvX7WfjBfgwDukQ1Z3FaCp2jQhv3jVNTYfRo3eFURETEm5ywVjJlWTafHToDwK1XxPPozT0JDnBRAfD1ddnltD9G5UNERMQFPtp3kumv53C6vIpmAb48dUtvxiS3/ekXNkEqHyIiIo2oxu7g+ff38eKGgxgGdI8JY3FaMh3aNDc7mmlUPkRERH5KA1eDLSw5y5Sl2Xxx5GsAbrsygYdG9iDI3/XzLNyJyoeIiMiPaeBqsB/uOcGMN3L4uqKa5oF+zB3bm5F9Yl0Q2P2pfIiIiFxMA1aDrbY7ePbdvfz1o0MA9GobxuK0FNq1CnFVarenVW1FREQupAGrwR77uoLJS7PJPloMwB1Xt2fWTd0I9Gv6wyxa1VZERORy1XM12P/uPM4fV2yn5Gw1oUF+PDOuDzf2ct16KZ5E5UNERORCLnGV16r8Quau2cXfPzkMQFJ8CxZNSiY+olljpvNoKh8iIiIXcgmrvOaFR5FxtAXbSmqLx28HJXL/jd0I8NPqJT9G5UNERORCfmI12HVdB3L/TVMpLXEQHuzPc+OTGNYjyoSgnkflQ0RE5EK+XQ123LjayaXfFJBKX3+euv4u/pkyEoB+7VqycFIybVsEm5nWo+i8kIiIyMWctxrskRYxjP3lM3XF4+5rO7Ls91epeNSTznyIiIj8mG9Wg337jQ95YEclZQ4LEc38mXdrX4Z0jTQ7nUdS+RAREfkRldV2Zq/ZxdJtNsDCzxIjWDgxmejwILOjeSyVDxERkYs4cKKMjMws9hwvxWKBjOs6MXVoZ/x8NWvhcqh8iIhI42nggmzuYGXWMR5cnUtFlZ3WzQOYf2sygzq3NjtWk6DyISIijaOBC7KZraKqhkfe2snyrbW5r+7Yivm39iUyTMMszqLyISIizteABdncwb6iUtKXZLH/RBk+Fpg6tAsZ13fC18didrQmRQvLiYiIczVgQTazGYbB8q3HePitXCqrHUSGBrJgYjIDOrYyO5rH0MJyIiJinnouyGa2clsND67OZVV2PgCDO7fm+Vv70rp5oMnJmi6VDxERca5LXJDtkvdrRLsLraQvyeLQqXJ8fSzM+HkX7rm2Iz4aZmlUKh8iIuJcl7AgW732awSGYZC55Siz1+yiqsZBdFgQL6Ql0799hGmZvInKh4iIONdPLMhWN+dj8GDXZwNKK6uZtXIHa7fXnnm5rmsbnpvQl4iQAFPyeCOVDxERca6LLMgG1D4GmD/flMmmufklZGRmceR0BX4+Fu6/sSu/HdRBwywuplu0iYiI8523IFuduDhTLrM1DIN/fHqE1Bc/5cjpCtq2COb1Pwzg99dofocZdOZDREQaxzcLspl9h9OSs9XMfHM763KPAzCsexTPju9Di2YaZjGLyoeIiDQeX19TL6fdlldMxtIs8s6cxd/XwqwR3fnNwPZYLDrbYSaVDxERaXIMw+Dvnxxh7rrdVNsN4iOCWTQphaT4FmZHE1Q+RESkiSmuqOK+5dt5f3cRACN6RTN3bB/Cg/1NTibfUvkQEZEmY+tXXzM5M4uCkkoCfH14aGR3fnlVOw2zuBmVDxER8XgOh8Ermw7xzLt7sTsM2rdqxqK0FHq1DTc7mlxAvS61nTNnDv379yc0NJTIyEjGjBnD3r17z9lnyJAhWCyWc37uvvtup4YWERH51pnyKu78xxfMXbcHu8NgVFIsayYPUvFwY/U687Fx40bS09Pp378/NTU1PPDAAwwfPpxdu3YREhJSt9/vfvc7HnvssbrHzZo1c15iERGRb2w5fIYpS7M5bq0k0M+HR2/uycT+8RpmcXP1Kh/vvPPOOY9fe+01IiMj2bp1K9dcc03d9mbNmhEdHe2chCIiIudxOAxe3HCAee/tw2FAhzYhLE5LoXvMjy/lLu7hsu5wWlJSAkBExLkL8SxZsoTWrVvTq1cvZs2aRUVFxUWPYbPZsFqt5/yIiIhczMlSG7e/uoVn/1tbPFKT27ImY5CKhwdp8IRTh8PBtGnTGDhwIL169arbnpaWRrt27YiNjWX79u386U9/Yu/evaxcufKCx5kzZw6zZ89uaAwREfEinx44xdTXczhZaiPI34fHRvdifL84DbN4GIthXGjJwZ92zz33sG7dOj7++GPi4uIuut8HH3zA0KFDOXDgAB07dvzB8zabDZvNVvfYarUSHx9PSUkJYWFqsSIiAnaHwcL1+1n4wX4MAzpHNufF21LoHBVqdjT5htVqJTw8/JK+vxt05iMjI4O1a9fy0Ucf/WjxALjyyisBLlo+AgMDCQwMbEgMERHxAieslUxdlsPmQ6cBmHBFHLNv7kVwgOtXxRXnqFf5MAyDyZMns2rVKjZs2EBiYuJPviYnJweAmJiYBgUUERHvtWn/Saa/nsOpsiqaBfjy5C29uCX5x//SK+6vXuUjPT2dzMxM3nrrLUJDQzl+vHaFwPDwcIKDgzl48CCZmZncdNNNtGrViu3btzN9+nSuueYa+vTp0yj/ASIi0vTU2B3Mf38/izccwDCgW3Qoi9JS6BTZ3Oxo4gT1mvNxsQk9r776KnfccQd5eXn88pe/JDc3l/LycuLj47nlllt48MEHL3n+Rn3GjEREpOkpLDnL1KU5bDlyBoC0KxN4eGQPgvw1zOLOGm3Ox0/1lPj4eDZu3FifQ4qIiNT5cM8JZryRw9cV1TQP9OOp1N7cnBRrdixxMq3tIiIipqu2O3j23b389aNDAPSMDWNxWgrtW4f8xCvFE6l8iIiIqfKLzzI5M4uso8UA3D6gHbNu6q5hliZM5UNEREzz3q4i7lu+jZKz1YQG+fH02D6M6K2rI5s6lQ8REXG5qhoHc9ft4e+fHAYgKS6cRWkpxEdoIVJvoPIhIiIulXemgozMLLYdq10f7K5Bifzpxm4E+F3WcmPiQVQ+RETEZd7JLeSPK7ZTWllDeLA/z45P4uc9osyOJS6m8iEiIo2ustrOnP/s5h+bvwIgJaEFCyclE9dSwyzeSOVDREQa1ZFT5aRnZrGzwArAH67twH3Du+Lvq2EWb6XyISIijWbNtgJmrdxBma2Gls38mTehL9d1izQ7lphM5UNERJyustrO7DW7WLrlKAD927dk4aRkYsKDTU4m7kDlQ0REnOrgyTLSl2Sx53gpFgukD+nEtGGd8dMwi3xD5UNERJxmVfYx/rwql4oqO61CApg/sS+DO7cxO5a4GZUPERG5bGer7Dz8Vi7Ltx4DYECHViyY2JfIsCCTk4k7UvkQEZHLsq+olPQlWew/UYbFAlOu78yUoZ3x9bGYHU3clMqHiIg0iGEYLN96jIffyqWy2kGb0EAW3NqXqzu1NjuauDmVDxERqbdyWw0Prc5lZXY+AIM7t2behL60CQ00OZl4ApUPERGpl92FVtIzszh0shwfC9w7vCv3XNsRHw2zyCVS+RARkUtiGAZLt+Tx6JqdVNU4iA4LYuGkZH6WGGF2NPEwKh8iIvKTSiureWBVLmu2FQAwpGsb5k3oS0RIgMnJxBOpfIiIyI/KzS8hIzOLI6cr8PWxcP8NXfnd4A4aZpEGU/kQEZELMgyD//vsK55Yu5squ4PY8CBeSEuhX7uWZkcTD6fyISIiP1BytpqZb25nXe5xAIZ1j+LZ8X1o0UzDLHL5VD5EROQc2/KKyViaRd6Zs/j7Wpg5ojt3DmyPxaJhFnEOlQ8REQFqh1n+/skR5q7bTbXdIK5lMIvTUkiKb2F2NGliVD5ERITiiiruW76d93cXAXBjz2j+Mq4P4cH+JieTpkjlQ0TEy2396mumLM0mv/gsAb4+PDiyO7+6qp2GWaTRqHyIiHgph8Pgfzcd4pl391LjMGjXqhmL01Lo1Tbc7GjSxKl8iIh4oTPlVdz7Rg4f7j0JwMg+McxJ7U1okIZZpPGpfIiIeJkth88wZWk2x62VBPj58Oionkz6WbyGWcRlVD5ERLyEw2Hw0saDzHtvH3aHQYc2ISxOS6F7TJjZ0cTLqHyIiHiBU2U2pr+ew6b9pwC4JbktT4zpRUigvgbE9fR/nYhIE/fpwVNMXZbDyVIbQf4+PDa6F+P7xWmYRUyj8iEi0kTZHQYvfLCfhev34zCgc2RzFt+WQpeoULOjiZdT+RARaYJOWCuZuiyHzYdOAzC+XxyzR/ekWYA+9sV8+r9QRKSJ2bT/JNNfz+FUWRXNAnx5YkwvUlPizI4lUkflQ0SkiaixO5j//n4WbziAYUC36FAWpaXQKbK52dFEzqHyISLSBBSWnGXq0hy2HDkDwKSfJfDIqB4E+fuanEzkh1Q+REQ83Id7TzDj9Ry+rqgmJMCXOWP7cHNSrNmxRC5K5UNExENV2x08+9+9/HXjIQB6xoaxKC2FxNYhJicT+XEqHyIiHii/+CyTM7PIOloMwK8HtOOBm7prmEU8gsqHiIiHeW9XEfct30bJ2WpCg/x4emwfRvSOMTuWyCVT+RAR8RBVNQ7+8s4e/vbxYQCS4sJ5YVIKCa2amZxMpH5UPkREPEDemQoyMrPYdqwEgDsHJjJzRDcC/HxMTiZSfyofIiJu7p3cQv64YjullTWEBfnx7PgkhveMNjuWSIOpfIiIuClbjZ2n/r2bf2z+CoDkhBa8MCmZuJYaZhHPpvIhIuKGjpwqJ2NpFrn5VgD+cG0H7hveFX9fDbOI51P5EBFxM2u2FTBr5Q7KbDW0bObPvAl9ua5bpNmxRJxG5UNExE1UVtt5bO0uMj8/CkD/9i1ZOCmZmPBgk5OJOJfKh4iIGzh4soz0JVnsOV6KxQL/M6Qj04d1wU/DLNIEqXyIiJhsVfYx/rwql4oqO61CAnj+1r5c06WN2bFEGo3Kh4iISc5W2Xnk7Vze+PIYAFd1iGDhxGQiw4JMTibSuFQ+RERMsL+olP9ZksX+E2VYLDDl+s5MGdoZXx+L2dFEGp3Kh4iICxmGwfKtx3j4rVwqqx20CQ1kwa19ubpTa7OjibiMyoeIiIuU22p4aHUuK7PzARjcuTXzJvSlTWigyclEXEvlQ0TEBXYXWsnIzOLgyXJ8LDDj5134nyGd8NEwi3ghlQ8RkUZkGAZLt+Qxe81ObDUOosICWTgxmSs7tDI7mohpVD5ERBpJaWU1D6zKZc22AgCGdG3Dc+OTaNVcwyzi3ep195o5c+bQv39/QkNDiYyMZMyYMezdu/ecfSorK0lPT6dVq1Y0b96csWPHUlRU5NTQIiLuLje/hFEvfMyabQX4+liYOaIbf7+9v4qHCPUsHxs3biQ9PZ3PPvuM9957j+rqaoYPH055eXndPtOnT2fNmjUsX76cjRs3UlBQQGpqqtODi4i4I8Mw+L/NR0h98VOOnK4gNjyIN/5wFXdf2/HS53fY7bBhAyxdWvtPu70xI4u4nMUwDKOhLz558iSRkZFs3LiRa665hpKSEtq0aUNmZibjxo0DYM+ePXTv3p3Nmzdz1VVX/eQxrVYr4eHhlJSUEBYW1tBoIiIuZ62sZuab2/nPjuMADOseybPjk2jRLODSD7JyJUydCseOfbctLg4WLAD9RU7cWH2+vy9rzkdJSQkAERERAGzdupXq6mqGDRtWt0+3bt1ISEi4aPmw2WzYbLZzwouIeJptecVkLM0i78xZ/H0t/OnGbtw1KBGLpR5Xs6xcCePGwfl/J8zPr92+YoUKiDQJDV6xyOFwMG3aNAYOHEivXr0AOH78OAEBAbRo0eKcfaOiojh+/PgFjzNnzhzCw8PrfuLj4xsaSUTE5QzD4O8fH2bcy5+Sd+YscS2DWX731fx2cIf6FQ+7vfaMx4VORn+7bdo0DcFIk9Dg8pGenk5ubi7Lli27rACzZs2ipKSk7icvL++yjici4irFFVX8/v+28tjaXVTbDW7sGc2/pwymb3yL+h9s06Zzh1rOZxiQl1e7n4iHa9CwS0ZGBmvXruWjjz4iLi6ubnt0dDRVVVUUFxefc/ajqKiI6OjoCx4rMDCQwEDN/hYRz5J19GsmZ2aTX3yWAF8f/vyL7vx6QLv6ne34vsJC5+4n4sbqdebDMAwyMjJYtWoVH3zwAYmJiec8369fP/z9/Vm/fn3dtr1793L06FEGDBjgnMQiIiZyOAxe+eggE17eTH7xWdq1asbK/7ma269u3/DiARAT49z9RNxYvc58pKenk5mZyVtvvUVoaGjdPI7w8HCCg4MJDw/nrrvuYsaMGURERBAWFsbkyZMZMGDAJV3pIiLizs6UV3Hf8m18sOcEACP7xDAntTehQf6Xf/DBg2uvasnPv/C8D4ul9vnBgy//vURMVq9LbS/W6l999VXuuOMOoPYmY/feey9Lly7FZrNxww038OKLL1502OV8utRWRNzRF0fOMDkzm+PWSgL8fHhkVA/SfpZweWc7zvft1S5wbgH59j10tYu4sfp8f1/WfT4ag8qHiLgTh8PgpY0HmffePuwOgw6tQ1iUlkKP2Eb6fLrQfT7i42H+fBUPcWsuu8+HiEhTdqrMxvTXc9i0/xQAtyS35YkxvQgJbMSPztRUGD269qqWwsLaOR6DB4Ovb+O9p4iLqXyISNNhtzvtS3vzwdNMXZbNiVIbQf4+PHZzL8ZfEefcYZaL8fWFIUMa/31ETKLyISJNg5NuS253GCz64AAL1u/DYUDnyOYsvi2FLlGhjRBaxDupfIiI53PSbclPlFYybVkOnx48DcD4fnHMHt2TZgH6qBRxJk04FRHPZrdD+/YXvzvot5eoHj78o0MwH+8/xbTXszlVVkWzAF+eGNOL1JS4i+4vIufShFMR8R71uS35BeZR1NgdLFi/n0UfHsAwoFt0KIvSUugU2bzxMot4OZUPEfFsl3Fb8uMllUxZls2Ww2cAmPSzBB4Z1YMgf11ZItKYVD5ExLM18LbkG/aeYMYb2zhTXkVIgC9zxvbh5qTYRggoIudT+RARz1bP25JX2x089999vLzxIAA9Y8NYlJZCYusQV6YW8Wr1WlhORMTt+PrWXk4L392G/FvfPp4/H3x9KSg+y8RXPqsrHr8e0I4377laxUPExVQ+RMTzpabWXk7btu252+Pi6i6zfX9XETct3MTWr74mNNCPF29L4bHRvTS/Q8QEGnYRkabhIrclrzIsPL12F//v48MA9IkLZ9GkFBJaNTM5sIj3UvkQkabjvNuS552pIGNpNtvyigG4c2AiM0d0I8BPJ31FzKTyISJN0ju5x7l/xTaslTWEBfnx7PgkhveMNjuWiKDyISJNjK3Gzpz/7OG1T48AkJzQghcmJRPXUsMsIu5C5UNEmoyvTpeTkZnNjvwSAP5wTQfuu6Er/r4aZhFxJyofItIkrN1ewMw3d1Bmq6FlM3+em5DE9d2izI4lIheg8iEiHq2y2s7ja3ex5POjAPRv35KFk5KJCQ82OZmIXIzKh4h4rEMny0jPzGZ3oRWLBf5nSEemD+uCn4ZZRNyayoeIeKTV2fk8sGoHFVV2WoUE8PytfbmmSxuzY4nIJVD5EBGPcrbKzqNv7+T1L/MAuKpDBAsmJhMVFmRyMhG5VCofIuIxDpwoJX1JNnuLSrFYYMr1nZkytDO+PpaffrGIuA2VDxHxCCu2HuOh1bmcrbbTJjSQBbf25epOrc2OJSINoPIhIm6toqqGB1fnsjIrH4BBnVrz/K19aRMaaHIyEWkolQ8RcVt7jltJX5LFwZPl+Fhgxs+7cM+QThpmEfFwKh8i4nYMw+D1L/J45O2d2GocRIUFsnBiMld2aGV2NBFxApUPEXErZbYa/rxqB2/lFABwbZc2zJuQRKvmGmYRaSpUPkTEbewsKCEjM5vDp8rx9bFw3/Cu/OGaDvhomEWkSVH5EBHTGYbBvz4/yuNrd1FV4yA2PIgX0pLp1y7C7Ggi0ghUPkTEVNbKama9uYN/7ygEYFj3SJ4Zl0TLkACTk4lIY1H5EBHTbD9WTEZmNkfPVODnY2HmiG7cNSgRi0XDLCJNmcqHiLicYRi89ukRnvrPbqrtBnEtg1mUlkLf+BZmRxMRF1D5EBGXKqmo5o8rtvHfXUUA3NAziqfHJREe7G9yMhFxFZUPEXGZ7KNfk5GZTX7xWQJ8ffjzL7rz6wHtNMwi4mVUPkSk0RmGwf/bdJi/vLOHGodBu1bNWDQphd5x4WZHExETqHyISKP6uryK+5ZvY/2eEwD8ok8Mc1J7ExakYRYRb6XyISKN5ssjZ5i8NJvCkkoC/Hx4eGQPbrsyQcMsIl5O5UNEnM7hMHj5o4M899992B0GHVqHsCgthR6xYWZHExE3oPIhIk51uszGjDe2sXHfSQDG9I3liVt60zxQHzciUkufBiLiNJ8dOs3UZdkUWW0E+fsw++aeTLgiXsMsInIOlQ8RuWx2h8HiDw8w//19OAzoFNmcxWkpdI0ONTuaiLghlQ8RuSwnSiuZ/noOnxw4DcC4fnE8NronzQL08SIiF6ZPBxFpsE8OnGLqshxOldkI9vfliTG9GNsvzuxYIuLmVD5EpN7sDoMF7+/jhQ8PYBjQNSqUxbcl0ylSwywi8tNUPkSkXoqslUxZms3nh88AMOln8TwyqidB/r4mJxMRT6HyISKXbOO+k0x/PYcz5VWEBPjyVGpvRvdta3YsEfEwKh8i8pNq7A6ee28fL204CECPmDAWpSXToU1zk5OJiCdS+RCRH1VQfJYpS7P58quvAfjVVe348y+6a5hFRBpM5UNELuqDPUXMeGMbxRXVhAb6MXdsH37RJ8bsWCLi4VQ+ROQHqu0Onn5nD/+76TAAvduGsygtmXatQkxOJiJNgcqHiJwj70wFk5dmk5NXDMBvBrZn5ohuBPppmEVEnEPlQ0TqvLvzOH9cvg1rZQ1hQX48Mz6JG3pGmx1LRJoYlQ8RwVZjZ+66Pbz6yREA+sa34IVJycRHNDM3mIg0SSofIl7uq9PlZGRmsyO/BIDfDU7kjzd0I8DPx+RkItJUqXyIeLF/by9k5pvbKbXV0KKZP8+NT2Jo9yizY4lIE6fyIeKFKqvtPPHvXfzrs6MAXNGuJQsnJRPbItjkZCLiDep9XvWjjz5i1KhRxMbGYrFYWL169TnP33HHHVgslnN+brzxRmflFZHLdPhUOakvflpXPP5nSEeW/v4qFQ8RcZl6n/koLy8nKSmJO++8k9TU1Avuc+ONN/Lqq6/WPQ4MDGx4QhFxmrdy8nlg5Q7Kq+xEhATw/K19ubZLG7NjiYiXqXf5GDFiBCNGjPjRfQIDA4mO1uV5Iu6istrOo2/vZNkXeQBcmRjBwknJRIUFmZxMRLxRo8z52LBhA5GRkbRs2ZLrr7+eJ554glatWl1wX5vNhs1mq3tstVobI5KI1zpwopT0JdnsLSrFYoHJ13dmyvWd8PPV1SwiYg6nl48bb7yR1NRUEhMTOXjwIA888AAjRoxg8+bN+Pr+8A6Jc+bMYfbs2c6OISLAiq3HeGh1Lmer7bRuHsiCiX0Z2Km12bFExMtZDMMwGvxii4VVq1YxZsyYi+5z6NAhOnbsyPvvv8/QoUN/8PyFznzEx8dTUlJCWFhYQ6OJeLWKqhoeWr2TN7OOATCwUyuev7UvkaEXGWax22HTJigshJgYGDwYLvCXBRGRi7FarYSHh1/S93ejX2rboUMHWrduzYEDBy5YPgIDAzUhVcSJ9h4vJT0ziwMnyvCxwLRhXUi/rhO+PpYLv2DlSpg6FY4d+25bXBwsWAAXmVQuInI5Gr18HDt2jNOnTxMTo2W4RRqTYRi88WUeD7+1E1uNg6iwQBZMTOaqDheebwXUFo9x4+D8E6D5+bXbV6xQARERp6t3+SgrK+PAgQN1jw8fPkxOTg4RERFEREQwe/Zsxo4dS3R0NAcPHuT++++nU6dO3HDDDU4NLiLfKbPV8OCqHazOKQDgmi5teH5CEq2a/8hZRbu99ozHhUZeDQMsFpg2DUaP1hCMiDhVvcvHl19+yXXXXVf3eMaMGQDcfvvtvPTSS2zfvp1//OMfFBcXExsby/Dhw3n88cc1tCLSSHYVWMnIzOLQqXJ8fSzcO7wLd1/TEZ+LDbN8a9Omc4dazmcYkJdXu9+QIU7NLCLerd7lY8iQIfzYHNV33333sgKJyKUxDIMlnx/lsbW7qKpxEBMexAuTkrmifcSlHaCw0Ln7iYhcIq3tIuKBrJXVzFq5g39vry0GQ7tF8uz4JFqGBFz6QS51Hpbma4mIk6l8iHiYHcdKSM/M4uiZCvx8LMwc0Y27BiVisfzEMMv5Bg+uvaolP//C8z4sltrnBw92TnARkW/oFociHsIwDF775DBjX/qUo2cqaNsimOV3D+C3gzvUv3hA7STSBQtq//3813/7eP58TTYVEadT+RDxACUV1dz9r608umYXVXYHw3tE8Z8pg0lOaHl5B05Nrb2ctm3bc7fHxekyWxFpNBp2EXFz2Ue/ZvLSbI59fRZ/XwsP3NSdO65u37CzHReSmlp7Oa3ucCoiLqLyIeKmDMPgbx8fZu66PdQ4DBIimrEoLZk+cS2c/2a+vrqcVkRcRuVDxA19XV7Ffcu3sX7PCQB+0TuGOWN7Exbkb3IyEZHLp/Ih4ma2fnWGyZnZFJRUEuDnw8Mje3DblQnOG2YRETGZyoeIm3A4DP760SGe/e9e7A6DxNYhLEpLpmdsuNnRREScSuVDxA2cLrMx441tbNx3EoDRfWN58pbeNA/UH1ERaXr0ySZiss8PnWbKsmyKrDYC/Xx4bHRPJlwRr2EWEWmyVD5ETGJ3GLz44QGef38fDgM6tgnhxdv60TU61OxoIiKNSuVDxAQnS21Mez2bTw6cBmBsShyPj+lJswD9kRSRpk+fdCIu9smBU0xdlsOpMhvB/r48PqYX4/rFmR1LRMRlVD5ELofdfsl3BrU7DBas388LH+zHMKBrVCiL0pLpHKVhFhHxLiofIg21ciVMnQrHjn23LS6udrG289ZEKbJWMnVZNp8dOgPAxP7xPDKqJ8EBuoW5iHgflQ+Rhli5EsaN++FS9Pn5tdu/tyjbxn0nmfF6DqfLqwgJ8OWp1N6M7tv2AgcVEfEOFsM4/9PTXFarlfDwcEpKSggLCzM7jsgP2e3Qvv25Zzy+z2KBuDhqDhzkuQ8O8tKGgwB0jwljcVoyHdo0d11WEREXqc/3t858iNTXpk0XLx4AhkFB8VmmzHufL4sdAPzyqgQe/EUPgvw1zCIiovIhUl+FhT/69AcdrmDGyBkUFzsIDfRjztjejOwT66JwIiLuT+VDpL5iYi64udrHl2eu+TWvXDkWgN5hPiz6wyDatQpxZToREben8iFSX4MH117Vkp9fN+H0WFgbMkb/iZzYbgDcsfcDZi2bS2Cgv5lJRUTcko/ZAUQ8jq9v7eW0ABYL/+10JTf95gVyYrsRVlnGX1c+yaO/GqjiISJyETrzIdIQqalUvbGCOf/6mFe7DQWgb8EeXvji/4if+/AP7vMhIiLfUfkQaYCjpyvIOB7J9m+Kx+/a2Pjj8BQCXp120TuciohILZUPkXr6z45C/rRiO6W2Glo08+e58UkM7R5ldiwREY+h8iFyiSqr7Tz5793832dfAXBFu5YsnJRMbItgk5OJiHgWlQ+RS3D4VDnpS7LYVWgF4J4hHZnx8y74+2rOtohIfal8iPyEt3LyeWDlDsqr7ESEBDBvQhJDukaaHUtExGOpfIhcRGW1ndlrdrJ0Sx4AP0uMYOHEZKLDg0xOJiLi2VQ+RC7gwIky0pdksbeoFIsFJl/XiSlDO+OnYRYRkcum8iFynje3HuPB1bmcrbbTunkg82/ty6DOrc2OJSLSZKh8iHyjoqqGh9/ayYqttSvWXt2xFfMn9iUyVMMsIiLOpPIhAuwrKiV9SRb7T5ThY4Fpw7qQfl0nfH0sZkcTEWlyVD7EqxmGwRtf5vHI2zuprHYQGRrIwknJXNWhldnRRESaLJUP8VplthoeXLWD1TkFAFzTpQ3zJiTRunmgyclERJo2lQ/xSrsKrGRkZnHoVDm+PhbuHd6Fu6/piI+GWUREGp3Kh3gVwzDI3HKU2Wt2UVXjICY8iIWTkunfPsLsaCIiXkPlQ7xGaWU1M1fu4N/bCwG4vlskz41PomVIgMnJRES8i8qHeIXc/BLSM7P46nQFfj4W/nRjN+4alKhhFhERE6h8SJNmGAb/3PwVT/57N1V2B21bBPNCWjIpCS3NjiYi4rVUPqTJKjlbzZ9WbOednccBGN4jimfGJRHezN/kZCIi3k3lQ5qknLxiMjKzOPb1Wfx9LTxwU3fuuLo9FouGWUREzKbyIU2KYRj87ePDzF23hxqHQUJEMxalJdMnroXZ0URE5BsqH9JkFFdUcd/ybby/+wQAN/WOZu7YPoQFaZhFRMSdqHxIk7D1qzNMzsymoKSSAD8fHhrZg19emaBhFhERN6TyIR7N4TB4ZdMhnnl3L3aHQWLrEBalJdMzNtzsaCIichEqH+KxTpfZuHf5NjbsPQnAzUmxPJXam+aB+t9aRMSd6VNaPNLnh04zZVk2RVYbgX4+zL65J7f2j9cwi4iIB1D5EI/icBi8uOEA897bh8OAjm1CWHxbCt2iw8yOJiIil0jlQzzGyVIbM97IYdP+UwCkprTl8dG9CNEwi4iIR9GntniETw+cYurrOZwstRHs78tjo3sy/op4s2OJiEgDqHyIW7M7DBau38/CD/ZjGNAlqjmL01LoHBVqdjQREWkglQ9xWyeslUxZls1nh84AcOsV8Tx6c0+CA3xNTiYiIpdD5UPc0kf7TjL99RxOl1fRLMCXp27pzZjktmbHEhERJ1D5ELdSY3fw/Pv7eHHDQQwDuseEsTgtmQ5tmpsdTUREnETlQ9xGYclZpi7NYcuR2mGW265M4KGRPQjy1zCLiEhT4lPfF3z00UeMGjWK2NhYLBYLq1evPud5wzB4+OGHiYmJITg4mGHDhrF//35n5ZUm6sM9J7hpwSa2HDlD80A/FqUl8+QtvVU8RESaoHqXj/LycpKSkli8ePEFn3/66adZuHAhL7/8Mp9//jkhISHccMMNVFZWXnZYaXqq7Q7m/Gc3v3ntC76uqKZX2zDWTh7EyD6xZkcTEZFGUu9hlxEjRjBixIgLPmcYBvPnz+fBBx9k9OjRAPzzn/8kKiqK1atXM3HixMtLK01KfvFZJmdmkXW0GIA7rm7PrJu6Eeinsx0iIk2ZU+d8HD58mOPHjzNs2LC6beHh4Vx55ZVs3rz5guXDZrNhs9nqHlutVmdGEjf1353H+eOK7ZScrSY0yI9nxvXhxl4xZscSEREXcGr5OH78OABRUVHnbI+Kiqp77nxz5sxh9uzZzowhbqyqxsHcdXv4+yeHAUiKb8GiScnERzQzOZmIiLhKved8ONusWbMoKSmp+8nLyzM7kjSSvDMVjH/507ri8dtBiSz/wwAVDxERL+PUMx/R0dEAFBUVERPz3Sn0oqIi+vbte8HXBAYGEhgY6MwY4obW7Sjk/je3U1pZQ3iwP8+NT2JYj6iffqGIiDQ5Tj3zkZiYSHR0NOvXr6/bZrVa+fzzzxkwYIAz30o8RGW1nYffyuWeJVmUVtbQr11L/jN1sIqHiIgXq/eZj7KyMg4cOFD3+PDhw+Tk5BAREUFCQgLTpk3jiSeeoHPnziQmJvLQQw8RGxvLmDFjnJlbPMCRU+WkZ2axs6B2EvHd13bk3uFd8Pc1fbRPRERMVO/y8eWXX3LdddfVPZ4xYwYAt99+O6+99hr3338/5eXl/P73v6e4uJhBgwbxzjvvEBQU5LzU4vbe3lbAAyt3UGarISIkgOcmJHFd10izY4mIiBuwGIZhmB3i+6xWK+Hh4ZSUlBAWFmZ2HKmnymo7s9fsYumWowD8rH0ECyclEx2u8iki0pTV5/tba7uI0xw4UUZGZhZ7jpdisUDGdZ2YOrQzfhpmERGR71H5EKdYmXWMB1fnUlFlp3XzAJ6/tS+DO7cxO5aIiLghlQ+5LBVVNTzy1k6Wbz0GwIAOrVgwsS+RYRpmERGRC1P5kAbbV1RK+pIs9p8ow8cCU4d2IeP6Tvj6WMyOJiIibkzlQ+rNMAyWbz3Gw2/lUlntoE1oIAsnJjOgYyuzo4mIiAdQ+ZB6KbfV8ODqXFZl5wMwuHNrnr+1L62b1/MutXY7bNoEhYUQEwODB4OvVrMVEfEGKh9yyXYXWknPzOLQyXJ8LHDv8K7cc21HfOo7zLJyJUydCseOfbctLg4WLIDUVOeGFhERt6PyIT/JMAwytxxl9ppdVNU4iA4LYuGkZH6WGFH/g61cCePGwfm3l8nPr92+YoUKiIhIE6ebjMmPKq2sZtbKHazdXgjAdV3b8NyEvkSEBNT/YHY7tG9/7hmP77NYas+AHD6sIRgREQ+jm4yJU+Tml5CRmcWR0xX4+Vi4/8au/HZQh/oPs3xr06aLFw+oPRuSl1e735AhDXsPERFxeyof8gOGYfDPzV/x5L93U2V30LZFMAsnJdOvXcvLO3BhoXP3ExERj6TyIecoOVvNzDe3sy73OADDukfx7Pg+tGjWgGGW88XEOHc/ERHxSCofUmdbXjEZS7PIO3MWf18Ls0Z05zcD22OxOOmmYYMH187pyM//4YRT+G7Ox+DBznk/ERFxS1rxSzAMg799fJhxL39K3pmzxEcEs+Luq7lzUKLzigfUTiJdsKD2388/7reP58/XZFMRkSZO5cPLFVdU8bt/buXxtbuothuM6BXN2smDSYpv0ThvmJpaezlt27bnbo+L02W2IiJeQsMuXmzrV18zOTOLgpJKAnx9eHBkd351VTvnnu24kNRUGD1adzgVEfFSKh9eyOEweGXTIZ55dy92h0H7Vs1YlJZCr7bhrgvh66vLaUVEvJTKh5c5U17FjDdy2LD3JACjkmJ56pZehAb5m5xMRES8hcqHF9ly+AxTlmZz3FpJoJ8Pj4zqyaSfxTf+MIuIiMj3qHx4AYfD4MUNB5j33j4cBnRoE8LitBS6x+j29SIi4noqH03cyVIbM97IYdP+UwCkJrfl8TG9CAnUr15ERMyhb6Am7NMDp5j6eg4nS20E+fvw2OhejO8Xp2EWERExlcpHE2R3GCxcv5+FH+zHMKBzZHMW35ZCl6hQs6OJiIiofDQ1J6yVTF2Ww+ZDpwGYcEUcs2/uRXCA7qEhIiLuQeWjCdm0/yTTX8/hVFkVzQJ8efKWXtySHGd2LBERkXOofDQBNXYH89/fz+INBzAM6BYdyqK0FDpFNjc7moiIyA+ofHi4wpKzTF2aw5YjZwBIuzKBh0f2IMhfwywiIuKeVD48jd1etybKh35tmLHbwdcV1TQP9OOp1N7cnBRrdkIREZEfpVVtPcnKldC+PdVDhzHnlff4zVYbX1dU07OZg7WTB6l4iIiIR1D58BQrV8K4ceSX2Lg1bS5/vXIsALdvXcObj4+j/UfvmhxQRETk0lgMwzDMDvF9VquV8PBwSkpKCAvT7b+B2qGW9u15LyiW+26aTklwKKGVZTy9biEj9n0KFgvExcHhw1qWXkRETFGf72/N+fAAVRs/Ym6XG/l7/zEAJBXs44W3/0JCSVHtDoYBeXm1c0G0TL2IiLg5lQ83l3emgoxNpWz7pnjc+cVqZm54jQBHzQ93Lix0bTgREZEGUPlwY+/kFvLHFdsprfQl/Gwpz/7neX5+YMvFXxAT47pwIiIiDaTy4YYqq+3M+c9u/rH5KwBSElqwcOEDxB3cceEXfDvnY/BgF6YUERFpGJUPN3PkVDnpmVnsLLAC8IdrO3Df8K74Rz8C48bVFo3vzxH+doXa+fM12VRERDyCLrV1I2u2FTDyhY/ZWWClZTN/Xr2jP7NGdMff1wdSU2HFCmjb9twXxcXVbk9NNSe0iIhIPenMhxuorLbz2NpdZH5+FID+7VuycFIyMeHB5+6YmgqjR9fd4ZSYmNqhFp3xEBERD6LyYbKDJ8tIX5LFnuOlWCyQPqQT04Z1xs/3IielfH11Oa2IiHg0lQ8Trco+xp9X5VJRZadVSADzJ/ZlcOc2ZscSERFpVCofJjhbZeeRt3N548tjAAzo0IoFE/sSGRZkcjIREZHGp/LhYvuKSklfksX+E2VYLDB1aGcmX98ZXx+L2dFERERcQuXDRQzDYPnWYzz8Vi6V1Q7ahAayYGJfru7Y2uxoIiIiLqXy4QLlthoeWp3Lyux8AAZ3bs28CX1pExpocjIRERHXU/loZLsLraRnZnHoZDk+Frh3eFfuubYjPhpmERERL6Xy0UgMw2Dpljxmr9mJrcZBdFgQCycl87PECLOjiYiImErloxGUVlbzwKpc1mwrAGBI1zbMm9CXiJAAk5OJiIiYT+XDyXLzS8jIzOLI6Qp8fSzcf0NXfje4g4ZZREREvqHy4SSGYfB/n33FE2t3U2V30LZFMAsnJdOvXUuzo4mIiLgV7ykfdnujrYlScraaWSu3858dxwEY1j2KZ8f3oUUzDbOIiIiczzvKx8qVMHUqHDv23ba4OFiw4LJXg92WV0zG0izyzpzF39fCzBHduXNgeywWDbOIiIhcSNMvHytXwrhxYBjnbs/Pr93ewOXoDcPg758cYe663VTbDeJaBrM4LYWk+BbOyS0iItJEWQzj/G9lc1mtVsLDwykpKSEsLOzyDma3Q/v2557x+D6LpfYMyOHD9RqCKa6o4r7l23l/dxEAN/aM5i/j+hAe7H95eUVERDxUfb6/m/aZj02bLl48oPZsSF5e7X6XuEz91q++ZsrSbPKLzxLg68ODI7vzq6vaaZhFRETkEjXt8lFY6LT9HA6D/910iGfe3UuNw6Bdq2YsTkuhV9vwywwpIiLiXZp2+YiJccp+Z8qruPeNHD7cexKAkX1imJPam9AgDbOIiIjUl4+zD/joo49isVjO+enWrZuz3+bSDB5cO6fjYkMiFgvEx9fudxFbDp/hpgWb+HDvSQL8fHjqlt68MClZxUNERKSBGuXMR8+ePXn//fe/exM/k06w+PrWXk47blxt0fj+3NpvC8n8+RecbOpwGLy08SDz3tuH3WHQoU0Ii9NS6B5zmZNgRUREvFyjtAI/Pz+io6Mb49D1l5paeznthe7zMX/+BS+zPVVmY/rrOWzafwqAW5Lb8sSYXoQENu1RKhEREVdolG/T/fv3ExsbS1BQEAMGDGDOnDkkJCRccF+bzYbNZqt7bLVanR8oNRVGj76kO5xuPniaqcuyOVFqI8jfh8dG92J8vzhdzSIiIuIkTr/Px7p16ygrK6Nr164UFhYye/Zs8vPzyc3NJTQ09Af7P/roo8yePfsH251yn496sDsMXvhgPwvX78dhQOfI5iy+LYUuUT/MLCIiIueqz30+Gv0mY8XFxbRr14558+Zx1113/eD5C535iI+Pd2n5OGGtZNrrOXx68DQAE66IY/bNvQgOcM7aLyIiIk2dW91krEWLFnTp0oUDBw5c8PnAwEACAwMbO8ZFbdp/kumv53CqrIpmAb48MaYXqSlxpuURERFp6px+qe35ysrKOHjwIDGXes8NF6mxO3j23b38+u9bOFVWRbfoUN7OGKTiISIi0sicfubjvvvuY9SoUbRr146CggIeeeQRfH19mTRpkrPfqsGOl1QyZWk2W46cASDtygQeHtmDIH8Ns4iIiDQ2p5ePY8eOMWnSJE6fPk2bNm0YNGgQn332GW3atHH2WzXIh3tPcO8b2zhTXkXzQD+eSu3NzUmxZscSERHxGk4vH8uWLXP2IZ2i2u7g2f/u5a8bDwHQMzaMxWkptG8dYnIyERER7+I1d81av7uornjcPqAds27qrmEWERERE3hN+bihZzS/vCqBgR1bM6K3e01+FRER8SZeUz4sFgtPjOltdgwRERGv1+iX2oqIiIh8n8qHiIiIuJTKh4iIiLiUyoeIiIi4lMqHiIiIuJTKh4iIiLiUyoeIiIi4lMqHiIiIuJTKh4iIiLiUyoeIiIi4lMqHiIiIuJTKh4iIiLiUyoeIiIi4lNutamsYBgBWq9XkJCIiInKpvv3e/vZ7/Me4XfkoLS0FID4+3uQkIiIiUl+lpaWEh4f/6D4W41Iqigs5HA4KCgoIDQ3FYrGYHcctWa1W4uPjycvLIywszOw4Xk+/D/ei34f70e/EvTTW78MwDEpLS4mNjcXH58dndbjdmQ8fHx/i4uLMjuERwsLC9AfZjej34V70+3A/+p24l8b4ffzUGY9vacKpiIiIuJTKh4iIiLiUyocHCgwM5JFHHiEwMNDsKIJ+H+5Gvw/3o9+Je3GH34fbTTgVERGRpk1nPkRERMSlVD5ERETEpVQ+RERExKVUPkRERMSlVD48yJw5c+jfvz+hoaFERkYyZswY9u7da3Ys+cbcuXOxWCxMmzbN7CheKz8/n1/+8pe0atWK4OBgevfuzZdffml2LK9kt9t56KGHSExMJDg4mI4dO/L4449f0rof4hwfffQRo0aNIjY2FovFwurVq8953jAMHn74YWJiYggODmbYsGHs37/fJdlUPjzIxo0bSU9P57PPPuO9996jurqa4cOHU15ebnY0r/fFF1/w17/+lT59+pgdxWt9/fXXDBw4EH9/f9atW8euXbt47rnnaNmypdnRvNJf/vIXXnrpJRYtWsTu3bv5y1/+wtNPP80LL7xgdjSvUV5eTlJSEosXL77g808//TQLFy7k5Zdf5vPPPyckJIQbbriBysrKRs+mS2092MmTJ4mMjGTjxo1cc801ZsfxWmVlZaSkpPDiiy/yxBNP0LdvX+bPn292LK8zc+ZMPvnkEzZt2mR2FAFGjhxJVFQUf/vb3+q2jR07luDgYP71r3+ZmMw7WSwWVq1axZgxY4Dasx6xsbHce++93HfffQCUlJQQFRXFa6+9xsSJExs1j858eLCSkhIAIiIiTE7i3dLT0/nFL37BsGHDzI7i1d5++22uuOIKxo8fT2RkJMnJyfzv//6v2bG81tVXX8369evZt28fANu2bePjjz9mxIgRJicTgMOHD3P8+PFzPrfCw8O58sor2bx5c6O/v9stLCeXxuFwMG3aNAYOHEivXr3MjuO1li1bRlZWFl988YXZUbzeoUOHeOmll5gxYwYPPPAAX3zxBVOmTCEgIIDbb7/d7HheZ+bMmVitVrp164avry92u50nn3yS2267zexoAhw/fhyAqKioc7ZHRUXVPdeYVD48VHp6Orm5uXz88cdmR/FaeXl5TJ06lffee4+goCCz43g9h8PBFVdcwVNPPQVAcnIyubm5vPzyyyofJnjjjTdYsmQJmZmZ9OzZk5ycHKZNm0ZsbKx+H6JhF0+UkZHB2rVr+fDDD4mLizM7jtfaunUrJ06cICUlBT8/P/z8/Ni4cSMLFy7Ez88Pu91udkSvEhMTQ48ePc7Z1r17d44ePWpSIu/2xz/+kZkzZzJx4kR69+7Nr371K6ZPn86cOXPMjiZAdHQ0AEVFRedsLyoqqnuuMal8eBDDMMjIyGDVqlV88MEHJCYmmh3Jqw0dOpQdO3aQk5NT93PFFVdw2223kZOTg6+vr9kRvcrAgQN/cOn5vn37aNeunUmJvFtFRQU+Pud+xfj6+uJwOExKJN+XmJhIdHQ069evr9tmtVr5/PPPGTBgQKO/v4ZdPEh6ejqZmZm89dZbhIaG1o3LhYeHExwcbHI67xMaGvqD+TYhISG0atVK83BMMH36dK6++mqeeuopJkyYwJYtW3jllVd45ZVXzI7mlUaNGsWTTz5JQkICPXv2JDs7m3nz5nHnnXeaHc1rlJWVceDAgbrHhw8fJicnh4iICBISEpg2bRpPPPEEnTt3JjExkYceeojY2Ni6K2IalSEeA7jgz6uvvmp2NPnGtddea0ydOtXsGF5rzZo1Rq9evYzAwECjW7duxiuvvGJ2JK9ltVqNqVOnGgkJCUZQUJDRoUMH489//rNhs9nMjuY1Pvzwwwt+Z9x+++2GYRiGw+EwHnroISMqKsoIDAw0hg4dauzdu9cl2XSfDxEREXEpzfkQERERl1L5EBEREZdS+RARERGXUvkQERERl1L5EBEREZdS+RARERGXUvkQERERl1L5EBEREZdS+RARERGXUvkQERERl1L5EBEREZdS+RARERGX+v8vwVNhCoxzqQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The following codes is to give a 2D plot our data and the linear function that we learnt from our setting\n",
    "plt.plot(x.transpose(), y, 'ro')\n",
    "plt.plot(x.transpose(), x.transpose()*h.w[0]+h.w[1], linestyle='solid')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
